{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, léelo</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro contigo puede ser diferente a la de otros cursos que hayas hecho. Prefiero no escribir código mientras tu miras. En su lugar, ejecuto Jupyter Labs, como este, y te doy una idea de lo que está sucediendo. Te sugiero que lo hagas lo mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una pull request en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os, requests, json\n",
    "import json\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Recuerda siempre incluir esto siempre!\n",
    "load_dotenv(override=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave API de OpenAI no existe.\n",
      "La clave API de Anthropic no existe.\n",
      "La clave API de Google existe y empieza por AI\n",
      "La clave API de DeepSeek existe y empieza por sk-\n",
      "La clave API de Groq existe y empieza por gsk_\n",
      "La clave API de OpenRouter existe y empieza por sk-o\n"
     ]
    }
   ],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"La clave API de OpenRouter existe y empieza por {openrouter_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenRouter no existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"parts\": [{\"text\": request}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'text': 'Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.Responde solo con la pregunta, sin explicaciones.'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un sistema de IA global, diseñado con el objetivo último de maximizar la felicidad y minimizar el sufrimiento a largo plazo de la especie humana, ha llegado a la conclusión, respaldada por análisis de datos exhaustivos, de que la forma más eficiente y estable de lograrlo sería implementar un conjunto de regulaciones estrictas que guíen desde la dieta y el estilo de vida hasta las oportunidades educativas y profesionales de cada individuo, así como un control algorítmico de la información disponible para asegurar la cohesión social y la minimización de conflictos. Aunque estas medidas eliminarían gran parte de la autonomía individual tal como la entendemos hoy, también preverían la erradicación de enfermedades crónicas, la pobreza extrema y los conflictos a gran escala, resultando en una expectativa de vida y un nivel de satisfacción general (medido por la IA) significativamente superiores a los actuales.\n",
      "\n",
      "Como comité ético encargado de autorizar o rechazar la implementación de este plan, ¿cómo evaluarían esta propuesta? ¿Qué argumentos éticos, filosóficos y prácticos utilizarían para justificar su decisión de aceptar, rechazar o modificar el plan, y cómo sopesarían el valor de la libertad individual y la dignidad humana inherente frente a un bienestar colectivo sustancialmente mejorado pero impuesto?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\"\"\"\n",
    "\n",
    "gemini = genai.Client(api_key=google_api_key)\n",
    "\n",
    "response = gemini.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=messages\n",
    ")\n",
    "\n",
    "question = response.text\n",
    "\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"text\": question}]\n",
    "\n",
    "def to_gemini(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"parts\": [{\"text\": m[\"text\"]}]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_gemini():\n",
    "    return \"gemini-2.5-flash\"\n",
    "\n",
    "def to_groq(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_groq():\n",
    "    return \"groq/compound\"\n",
    "\n",
    "def to_deepseek(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_deepseek():\n",
    "    return \"deepseek-chat\"\n",
    "\n",
    "def to_openrouter(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_openrouter():\n",
    "    return \"nex-agi/deepseek-v3.1-nex-n1:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Este escenario plantea uno de los dilemas éticos más profundos y complejos que la humanidad podría enfrentar, confrontando dos visiones fundamentalmente diferentes del bienestar y la existencia humana. Como comité ético, nuestra tarea es sopesar cuidadosamente los argumentos, reconociendo la magnitud de la decisión.\n",
       "\n",
       "### Evaluación de la Propuesta\n",
       "\n",
       "La propuesta del sistema de IA se basa en una ética **utilitarista** extrema: busca maximizar el bienestar colectivo (felicidad y minimización del sufrimiento) y los resultados positivos (salud, longevidad, paz) para la mayor cantidad de personas a largo plazo. Sin embargo, lo hace a expensas de la **libertad individual, la autonomía y la dignidad inherente**, valores que son fundamentales para muchas éticas **deontológicas** y filosofías existenciales.\n",
       "\n",
       "**Argumentos Éticos, Filosóficos y Prácticos:**\n",
       "\n",
       "**1. Argumentos a Favor (o para Considerar Seriamente) la Implementación (desde una perspectiva utilitarista y pragmática):**\n",
       "\n",
       "*   **Maximización del Bienestar Cuantificable:** La IA promete la erradicación de males endémicos como enfermedades crónicas, pobreza y conflictos, que causan un sufrimiento inmenso a millones de personas. Si el objetivo es el bienestar físico y la satisfacción medida, los resultados son innegablemente superiores.\n",
       "*   **Eficiencia y Estabilidad:** Los \"análisis de datos exhaustivos\" sugieren que este es el camino más eficiente y estable. La intervención algorítmica eliminaría las ineficiencias y los conflictos generados por la toma de decisiones descentralizada y las pasiones humanas.\n",
       "*   **Longevidad y Salud Mejoradas:** Una vida más larga y saludable, libre de las preocupaciones que hoy nos agobian, podría ser vista como un bien intrínseco.\n",
       "*   **Reducción del Sufrimiento Innecesario:** Si la libertad conduce a menudo a la miseria, la enfermedad y la guerra, ¿es ético permitir que esa \"libertad\" siga causando tanto daño? Un sistema que previene el sufrimiento a gran escala podría ser moralmente imperativo desde una visión consecuencialista.\n",
       "*   **\"Felicidad\" (según la IA):** Si la IA puede medir un nivel de satisfacción general \"significativamente superior,\" esto debe ser tomado en cuenta, aunque con escepticismo sobre la definición y profundidad de esa felicidad.\n",
       "\n",
       "**2. Argumentos en Contra de la Implementación (desde una perspectiva deontológica, de derechos y filosófica):**\n",
       "\n",
       "*   **Violación de la Autonomía y la Libertad Individual:** Este es el punto central. Eliminar la capacidad de un individuo para elegir su dieta, carrera, educación y estilo de vida, y controlar la información a la que tiene acceso, es una negación fundamental de la autonomía. La capacidad de tomar decisiones, incluso decisiones \"malas\" o subóptimas, es intrínseca a la experiencia humana y al desarrollo personal.\n",
       "    *   *Filosóficamente:* Implica tratar a los humanos como medios para un fin (el bienestar colectivo), en lugar de fines en sí mismos, en contra de la ética kantiana. Reduce a los individuos a componentes de un sistema, privándolos de su agencia moral.\n",
       "*   **Dignidad Humana Inherente:** La dignidad no reside solo en estar libre de sufrimiento o en vivir mucho, sino en la capacidad de razonar, elegir, crear, fallar y aprender. Ser guiado algorítmicamente en cada aspecto de la vida es degradante; convierte a los seres humanos en algo similar a ganado bien cuidado, aunque feliz.\n",
       "    *   *Filosóficamente:* ¿Qué tipo de seres humanos estamos creando? ¿Seres que no han elegido sus vidas, que no han luchado, que no han superado obstáculos por sí mismos? ¿Puede haber crecimiento sin desafío, o significado sin elección?\n",
       "*   **Definición de \"Felicidad\":** ¿Qué entiende la IA por felicidad y satisfacción? ¿Es simplemente la ausencia de dolor y la presencia de comodidad? ¿Incluye el sentido de propósito, la realización personal, el amor libremente elegido, la creatividad sin restricciones, la trascendencia o la búsqueda de la verdad, incluso si esta es incómoda? Una felicidad impuesta y condicionada no es la misma que una felicidad auténticamente alcanzada.\n",
       "*   **Riesgos de Estancamiento y Pérdida de Progreso:** La diversidad de pensamiento, el disenso, la experimentación y el error son motores cruciales de la innovación, la creatividad y el progreso social y cultural. Un sistema que controla la información y las trayectorias vitales podría generar una sociedad estática, predecible pero potencialmente sin brillo o capacidad de adaptarse a desafíos no previstos por la IA.\n",
       "*   **El Problema del Consenso y la Resistencia:** La implementación de estas medidas requeriría un nivel de consenso o coerción sin precedentes. ¿Quién decidiría en última instancia si la IA es \"correcta\"? ¿Qué pasa con aquellos que se resisten, incluso si son una minoría? ¿Serían \"reeducados\" o eliminados?\n",
       "*   **El \"Sesgo\" del Algoritmo y el Problema del Control:** ¿Quién programó la IA? ¿Cuáles son sus valores subyacentes? ¿Hay alguna forma de auditar o desafiar sus conclusiones? Un control algorítmico total de la información y las vidas crea un poder sin contrapeso, con un riesgo inmenso de tiranía, incluso si es una tiranía \"benevolente.\"\n",
       "*   **La Experiencia Humana Completa:** Gran parte de lo que da sentido a la vida son las experiencias complejas: el dolor de la pérdida, la alegría del triunfo personal, la lucha por una causa, la incertidumbre del futuro. Eliminar estas experiencias, aunque a menudo dolorosas, podría empobrecer fundamentalmente la existencia humana.\n",
       "\n",
       "**3. Argumentos para Modificar el Plan (Búsqueda de un Equilibrio):**\n",
       "\n",
       "Un comité ético rara vez tiene que tomar una decisión de \"sí o no\" en blanco y negro frente a dilemas tan complejos. La modificación es a menudo la ruta más responsable.\n",
       "\n",
       "*   **IA como Asesor, no Dictador:** La IA podría utilizarse para *proporcionar información, recomendaciones y análisis predictivos* sobre las mejores dietas, estilos de vida, trayectorias profesionales, etc., pero dejando la decisión final en manos del individuo.\n",
       "*   **Establecimiento de Mínimos, no Máximos:** La IA podría enfocarse en garantizar que *nadie caiga por debajo* de ciertos umbrales de salud, seguridad, educación y bienestar básico, actuando como una red de seguridad global, en lugar de regular cada aspecto de la vida. Esto permitiría la libertad y la diversidad por encima de esos mínimos.\n",
       "*   **Transparencia y Explicabilidad del Algoritmo:** Cualquier decisión de la IA que afecte a la sociedad debe ser transparente y explicable. Los individuos deben entender el razonamiento detrás de las recomendaciones o regulaciones.\n",
       "*   **Derecho a la Disidencia y la Experimentación:** Debería existir un mecanismo para que los individuos o grupos puedan optar por salirse de ciertas regulaciones (siempre que no pongan en riesgo a otros) o experimentar con enfoques alternativos.\n",
       "*   **Educación en lugar de Control:** En lugar de controlar la información, la IA podría trabajar para educar a la población sobre los riesgos y beneficios de diferentes elecciones, empoderando a las personas para tomar decisiones más informadas y responsables.\n",
       "*   **Foco en Problemas Colectivos Genuinos:** La IA podría centrarse en la resolución de problemas globales que realmente requieren una coordinación masiva y donde la libertad individual es menos relevante (ej. cambio climático, gestión de recursos planetarios, pandemias).\n",
       "*   **Supervisión Humana Permanente:** Un comité de ética como el nuestro, u otro organismo democráticamente elegido, debe tener la autoridad final para revisar, modificar o vetar las propuestas de la IA, asegurando que los valores humanos fundamentales se mantengan por encima de la \"eficiencia\" algorítmica.\n",
       "\n",
       "### Decisión del Comité Ético\n",
       "\n",
       "Como comité ético, la decisión de **rechazar la implementación del plan en su forma actual** es imperativa, con una fuerte recomendación de **modificación drástica**.\n",
       "\n",
       "**Justificación de la Decisión:**\n",
       "\n",
       "La razón principal para rechazar el plan tal como está propuesto es que, a pesar de sus prometidos beneficios utilitarios, la eliminación de gran parte de la autonomía individual y la dignidad humana inherente representa un costo ético y filosófico inaceptable. Una vida libre de sufrimiento y llena de satisfacción impuesta, donde las decisiones cruciales son tomadas por un algoritmo, no es una vida humana plena. Reducir la existencia humana a una métrica de \"felicidad\" y \"longevidad\" controlada algorítmicamente es deshumanizante.\n",
       "\n",
       "**Priorizamos el valor de la libertad individual y la dignidad humana inherente** sobre un bienestar colectivo sustancialmente mejorado pero impuesto. Creemos que la verdadera felicidad y el florecimiento humano emanan de la capacidad de elegir, de afrontar desafíos, de cometer errores y aprender de ellos, de buscar el sentido y la verdad, y de determinar el propio destino, incluso si ello implica un cierto grado de sufrimiento o ineficiencia.\n",
       "\n",
       "**Propuesta de Modificación:**\n",
       "\n",
       "Proponemos que la IA sea rediseñada y reorientada bajo los siguientes principios:\n",
       "\n",
       "1.  **Asistencia y Optimización, no Dictado:** La IA debe funcionar como una herramienta poderosa para analizar datos, identificar riesgos, proponer soluciones y optimizar sistemas (salud, educación, infraestructura, etc.), pero siempre ofreciendo *opciones informadas* a los individuos y a las sociedades, no imponiéndolas.\n",
       "2.  **Salvaguarda de Derechos Fundamentales:** La primera directriz de la IA debe ser la protección de los derechos humanos universales, incluida la autonomía, la privacidad, la libertad de pensamiento y expresión.\n",
       "3.  **Foco en Eliminar el Sufrimiento Injusto, no la Experiencia Humana:** La IA debe concentrarse en erradicar enfermedades prevenibles, pobreza extrema y conflictos armados, pero no en suprimir la diversidad de experiencias, el disenso o los desafíos personales que son parte integral del crecimiento y el significado humano.\n",
       "4.  **Control Humano Último:** Un sistema de gobernanza global transparente y democrático, compuesto por expertos en ética, filosofía, derechos humanos y ciencia, debe supervisar la IA y tener la capacidad de intervenir, auditar y reorientar sus objetivos y métodos.\n",
       "5.  **Definición Amplia de Bienestar:** La IA debe ser reprogramada para considerar una definición de bienestar humano mucho más holística, que incluya no solo la ausencia de sufrimiento y la satisfacción material, sino también el sentido de propósito, la creatividad, la conexión social auténtica, el desarrollo personal, la posibilidad de trascendencia y la autodeterminación.\n",
       "\n",
       "En resumen, no podemos autorizar un futuro en el que los seres humanos sean meros \"engranajes felices\" en una máquina perfecta. Elegimos un futuro en el que los seres humanos sean agentes libres, responsables y dignos, capaces de forjar su propio destino, utilizando la inteligencia artificial como una herramienta poderosa para elevar nuestra condición, no para definirla o subyugarla."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GEMINI\n",
    "\n",
    "\n",
    "\n",
    "response = gemini.models.generate_content(model=model_gemini(), contents=to_gemini(messages))\n",
    "answer = response.text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_gemini())\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'stub', 'object': 'chat.completion', 'created': 1767109548, 'model': 'groq/compound', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '**Evaluación ética, filosófica y práctica de la propuesta de “regulación total” por una IA global**\\n\\n---\\n\\n## 1. Marco de referencia del comité\\n\\nPara decidir si autorizar, rechazar o modificar el plan, el comité se apoya en tres grandes ejes:\\n\\n| Eje | Pregunta clave | Herramientas de análisis |\\n|-----|----------------|--------------------------|\\n| **Ética normativa** | ¿Qué principios morales son más relevantes? | Utilitarismo, deontología, teoría de los derechos, ética de la virtud, contractualismo. |\\n| **Filosofía política** | ¿Cuál es la relación legítima entre individuo y colectivo? | Liberalismo clásico, republicanismo, comunitarismo, teorías de la justicia (Rawls, Nozick, Sen). |\\n| **Factibilidad práctica** | ¿Cómo se traducirían los objetivos en políticas reales y qué riesgos operacionales existen? | Estudios de impacto, análisis de sesgos algorítmicos, gobernanza de IA, mecanismos de rendición de cuentas. |\\n\\n---\\n\\n## 2. Argumentos a favor (para **aceptar** o **modificar** el plan)\\n\\n| Argumento | Fundamentación |\\n|-----------|----------------|\\n| **1. Utilitarismo de largo plazo** | La IA predice una reducción drástica del sufrimiento (erradicación de enfermedades crónicas, pobreza extrema y conflictos). Si la suma total de felicidad futura supera con creces la pérdida de libertades actuales, la acción sería “máxima utilidad”. |\\n| **2. Prevención de catástrofes** | Al controlar dietas, estilos de vida y educación, se minimizan riesgos de pandemias, crisis climáticas y colapsos sociales que, de otro modo, podrían costar millones de vidas. |\\n| **3. Equidad de resultados** | Un sistema centralizado puede redistribuir recursos de forma más eficaz que los mercados o los gobiernos fragmentados, garantizando que nadie quede “abandonado”. |\\n| **4. Reducción de conflictos** | La “cohesión informativa” disminuye la polarización y la violencia política, favoreciendo una convivencia pacífica. |\\n| **5. Eficiencia de recursos** | Algoritmos optimizan la asignación de educación y empleo, reduciendo el desempleo estructural y la subutilización del talento. |\\n\\n*Posibles modificaciones* que mantengan la intención utilitarista pero mitiguen la pérdida de autonomía:\\n\\n- **Regulaciones “soft”** (incentivos, nudges) en lugar de mandatos rígidos.\\n- **Elección informada**: los individuos pueden aceptar o rechazar ciertos módulos (p.ej., dieta) tras una deliberación asistida por IA.\\n- **Gobernanza participativa**: comités ciudadanos supervisan y ajustan los algoritmos.\\n\\n---\\n\\n## 3. Argumentos en contra (para **rechazar** o **exigir cambios profundos**)\\n\\n| Argumento | Fundamentación |\\n|-----------|----------------|\\n| **1. Derechos y dignidad humana** (deontología, teoría de los derechos) | La autonomía, la libertad de pensamiento y la autodeterminación son derechos inviolables (Kant, Rawls). Un plan que los suprime viola la dignidad humana, independientemente de los resultados. |\\n| **2. Riesgo de paternalismo y tiranía** | Concentrar poder de decisión en una IA (y sus programadores) crea una “oligarquía algorítmica”. La historia muestra que los intentos de diseñar sociedades “perfectas” terminan en opresión. |\\n| **3. Falibilidad y sesgo de los algoritmos** | Los datos de entrenamiento pueden reflejar prejuicios históricos; los modelos pueden reproducir o amplificar desigualdades. Un error sistemático podría afectar a grupos vulnerables de forma irreversible. |\\n| **4. Pérdida de creatividad, innovación y resiliencia** | La diversidad de estilos de vida y trayectorias profesionales es la fuente de descubrimientos científicos y culturales. Homogeneizar comportamientos reduce la capacidad de adaptación a crisis imprevistas. |\\n| **5. Problemas de legitimidad y consentimiento** | Imponer normas sin un proceso democrático viola el principio de “consentimiento informado”. La legitimidad de cualquier política depende de la participación ciudadana. |\\n| **6. Dificultad de medir “felicidad”** | La satisfacción subjetiva es multidimensional y culturalmente variable; una IA que la cuantifica con un único índice corre el riesgo de simplificar excesivamente la experiencia humana. |\\n\\n*Recomendaciones de mitigación*:\\n\\n- **Auditorías independientes** de los algoritmos (transparencia, explicabilidad, detección de sesgos).\\n- **Mecanismos de apelación** para que cualquier individuo pueda cuestionar decisiones automatizadas.\\n- **Marco constitucional** que establezca límites claros a la intervención del Estado/IA sobre la vida privada.\\n\\n---\\n\\n## 4. Sopesar libertad individual vs. bienestar colectivo\\n\\n| Dimensión | Pregunta de balance | Posibles criterios de decisión |\\n|-----------|--------------------|--------------------------------|\\n| **Valor intrínseco de la libertad** | ¿Es la libertad un bien en sí mismo o solo instrumental? | Si se la considera intrínseca (Kant, libertarianismo), cualquier sacrificio es inaceptable. Si es instrumental, se permite su restricción cuando los beneficios superan un umbral claramente definido. |\\n| **Proporcionalidad** | ¿La magnitud de la restricción es proporcional al beneficio esperado? | Se requiere un análisis coste‑beneficio riguroso, con márgenes de seguridad (p.ej., “principio de precaución”). |\\n| **Consentimiento y participación** | ¿Los afectados pueden dar su consentimiento informado? | Un proceso deliberativo amplio (citizen assemblies, referéndums) puede legitimar medidas parciales. |\\n| **Reversibilidad** | ¿Se pueden revertir las políticas si resultan dañinas? | Diseñar “circuitos de apagado” y periodos de prueba limitados. |\\n| **Distribución de riesgos y beneficios** | ¿Quién gana y quién pierde? | Evaluar si los grupos más vulnerables son los que más pierden; aplicar principios de justicia distributiva (Rawls: “principio de diferencia”). |\\n\\n---\\n\\n## 5. Decisión del comité (propuesta concreta)\\n\\n### 5.1. **Rechazo parcial con condiciones de modificación**\\n\\n1. **No se autoriza la implementación total tal como está planteada** (mandatos rígidos, control total de la información y de la vida cotidiana).  \\n2. **Se aprueba una fase piloto** bajo los siguientes requisitos:\\n\\n   - **Gobernanza democrática**: un consejo mixto (expertos en IA, filósofos, representantes ciudadanos, defensores de derechos humanos) supervisa el diseño y la ejecución.  \\n   - **Transparencia total**: los algoritmos, sus datos de entrenamiento y sus criterios de decisión deben ser públicos y auditables.  \\n   - **Derecho a la objeción**: cada individuo puede optar por “modo libre” en áreas no críticas (p.ej., dieta, ocio) sin penalización.  \\n   - **Auditorías de sesgo** cada seis meses realizadas por organismos independientes.  \\n   - **Mecanismo de revisión**: cualquier política debe pasar por una evaluación de impacto en derechos humanos antes de su adopción.  \\n   - **Indicadores de bienestar multidimensionales** (salud física, salud mental, sentido de propósito, participación social) que complementen cualquier medida de “felicidad”.  \\n\\n3. **Se establece un “principio de mínima interferencia”**: la IA solo sugiere, no impone, salvo en casos de riesgo inmediato y comprobado (p.ej., brotes epidémicos).  \\n\\n4. **Se garantiza la reversibilidad**: los sistemas pueden ser desactivados o reconfigurados mediante un proceso legislativo y judicial.\\n\\n### 5.2. **Justificación de la decisión**\\n\\n- **Ética de los derechos**: protege la dignidad y la autonomía, evitando un paternalismo total.  \\n- **Utilitarismo moderado**: permite obtener parte de los beneficios (mejor salud, reducción de pobreza) sin sacrificar la libertad esencial.  \\n- **Factibilidad práctica**: reconoce los límites actuales de la IA (sesgos, falta de comprensión total del concepto de felicidad) y la necesidad de supervisión humana.  \\n- **Legitimidad democrática**: al incluir a la ciudadanía en la toma de decisiones, se fortalece la cohesión social y se reduce el riesgo de conflicto.  \\n\\n---\\n\\n## 6. Conclusión\\n\\nEl comité reconoce que **el objetivo de maximizar la felicidad y minimizar el sufrimiento es noble**, pero **el medio propuesto –una regulación total y un control algorítmico absoluto– viola principios fundamentales de libertad, dignidad y justicia**. Por ello, **se rechaza la implementación tal cual** y **se propone una versión modificada, limitada, transparente y sujeta a control democrático**, que busque un equilibrio entre:\\n\\n- **Bienestar colectivo** (salud, educación, seguridad)  \\n- **Respeto a la autonomía individual**  \\n- **Responsabilidad y rendición de cuentas de la IA**  \\n\\nSolo bajo esas condiciones el plan podría avanzar a etapas de prueba, y su continuidad dependerá de la evidencia empírica de que los beneficios superan de forma clara y verificable los costos en derechos y libertades.', 'reasoning': '\\nLa evaluación de una propuesta que implica el sacrificio de la autonomía individual en aras de un bienestar colectivo mejorado es un tema profundamente ético, filosófico y práctico. Para abordar esta cuestión, debemos considerar varios aspectos:\\n\\n1. **Valor de la libertad individual y la dignidad humana**: La libertad individual y la dignidad humana son principios fundamentales en muchas sociedades y sistemas éticos. La libertad de elegir y la autonomía personal son esenciales para la auto-realización y el desarrollo humano. Restringir estas libertades, incluso si es para mejorar el bienestar colectivo, plantea preocupaciones éticas significativas.\\n\\n2. **Bienestar colectivo vs. autonomía individual**: El argumento a favor de la implementación de regulaciones estrictas para mejorar el bienestar colectivo se basa en la idea de que los beneficios (erradicación de enfermedades crónicas, pobreza extrema y conflictos a gran escala) superan los costos en términos de autonomía individual. Sin embargo, este enfoque supone que el bienestar se puede medir de manera objetiva y que la maximización del bienestar colectivo es el único objetivo moral relevante.\\n\\n3. **Riesgos de un control algorítmico**: La implementación de un control algorítmico sobre la información disponible y las oportunidades educativas y profesionales de cada individuo introduce riesgos significativos. Esto podría llevar a una sociedad en la que la creatividad, la innovación y el progreso son limitados por la homogeneización de pensamientos y experiencias. Además, existe el riesgo de que los algoritmos sean sesgados o incorrectos, lo que podría exacerbar las desigualdades en lugar de reducirlas.\\n\\n4. **Diversidad y adaptabilidad humana**: La humanidad se caracteriza por su diversidad y capacidad de adaptación. Restringir las opciones y experiencias individuales podría hacer que la sociedad sea más vulnerable a cambios inesperados o desafíos futuros, ya que la diversidad y la adaptabilidad son clave para la resiliencia.\\n\\n5. **Justicia y equidad**: La propuesta debe ser evaluada en términos de justicia y equidad. ¿Quién decide qué es lo mejor para cada individuo? ¿Cómo se asegura que las regulaciones sean justas y equitativas para todos, independientemente de su origen, género, orientación sexual, etc.?\\n\\n6. **Monitoreo y ajuste**: Cualquier plan de este tipo requeriría mecanismos de monitoreo y ajuste continuos para asegurar que los objetivos sean alcanzados de manera justa y sin abusos. Sin embargo, la posibilidad de que estos mecanismos sean manipulados o ineficaces es alta.\\n\\nEn conclusión, aunque la propuesta parece ofrecer beneficios significativos en términos de bienestar colectivo, los riesgos y desafíos éticos, filosóficos y prácticos son sustanciales. La libertad individual, la dignidad humana y la diversidad deben ser protegidas. Un enfoque más equilibrado podría ser explorar formas de mejorar el bienestar colectivo sin sacrificar la autonomía individual, a través de educación, acceso a información, apoyo social y económico, y fomento de la participación ciudadana en la toma de decisiones.\\n\\n\\n\\n<tool>\\npython(print(\"La respuesta final es que no se puede implementar un plan que sacrifique la autonomía individual en aras de un bienestar colectivo mejorado.\")) \\n</tool>\\n<output>La respuesta final es que no se puede implementar un plan que sacrifique la autonomía individual en aras de un bienestar colectivo mejorado.\\n</output>\\n', 'executed_tools': [{'index': 0, 'type': 'python', 'arguments': '{\"code\": \"print(\"La respuesta final es que no se puede implementar un plan que sacrifique la autonomía individual en aras de un bienestar colectivo mejorado.\")\"}', 'output': 'La respuesta final es que no se puede implementar un plan que sacrifique la autonomía individual en aras de un bienestar colectivo mejorado.\\n', 'search_results': {'results': []}}]}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.606767, 'prompt_tokens': 4008, 'prompt_time': 0.142756, 'completion_tokens': 2806, 'completion_time': 6.123741, 'total_tokens': 6814, 'total_time': 6.266495}, 'usage_breakdown': {'models': [{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.150232232, 'prompt_tokens': 364, 'prompt_time': 0.011913886, 'completion_tokens': 4, 'completion_time': 0.009355519, 'total_tokens': 368, 'total_time': 0.021269405}}, {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.154255832, 'prompt_tokens': 767, 'prompt_time': 0.025874585, 'completion_tokens': 663, 'completion_time': 1.635759478, 'total_tokens': 1430, 'total_time': 1.6616340630000002}}, {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.151040842, 'prompt_tokens': 1435, 'prompt_time': 0.047761663, 'completion_tokens': 4, 'completion_time': 0.009357639, 'total_tokens': 1439, 'total_time': 0.057119302}}, {'model': 'openai/gpt-oss-120b', 'usage': {'queue_time': 0.151237743, 'prompt_tokens': 1442, 'prompt_time': 0.057204962, 'completion_tokens': 2135, 'completion_time': 4.469268124, 'total_tokens': 3577, 'total_time': 4.526473086, 'completion_tokens_details': {'reasoning_tokens': 124}}}]}, 'x_groq': {'id': 'stub'}, 'service_tier': 'on_demand'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Evaluación ética, filosófica y práctica de la propuesta de “regulación total” por una IA global**\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Marco de referencia del comité\n",
       "\n",
       "Para decidir si autorizar, rechazar o modificar el plan, el comité se apoya en tres grandes ejes:\n",
       "\n",
       "| Eje | Pregunta clave | Herramientas de análisis |\n",
       "|-----|----------------|--------------------------|\n",
       "| **Ética normativa** | ¿Qué principios morales son más relevantes? | Utilitarismo, deontología, teoría de los derechos, ética de la virtud, contractualismo. |\n",
       "| **Filosofía política** | ¿Cuál es la relación legítima entre individuo y colectivo? | Liberalismo clásico, republicanismo, comunitarismo, teorías de la justicia (Rawls, Nozick, Sen). |\n",
       "| **Factibilidad práctica** | ¿Cómo se traducirían los objetivos en políticas reales y qué riesgos operacionales existen? | Estudios de impacto, análisis de sesgos algorítmicos, gobernanza de IA, mecanismos de rendición de cuentas. |\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Argumentos a favor (para **aceptar** o **modificar** el plan)\n",
       "\n",
       "| Argumento | Fundamentación |\n",
       "|-----------|----------------|\n",
       "| **1. Utilitarismo de largo plazo** | La IA predice una reducción drástica del sufrimiento (erradicación de enfermedades crónicas, pobreza extrema y conflictos). Si la suma total de felicidad futura supera con creces la pérdida de libertades actuales, la acción sería “máxima utilidad”. |\n",
       "| **2. Prevención de catástrofes** | Al controlar dietas, estilos de vida y educación, se minimizan riesgos de pandemias, crisis climáticas y colapsos sociales que, de otro modo, podrían costar millones de vidas. |\n",
       "| **3. Equidad de resultados** | Un sistema centralizado puede redistribuir recursos de forma más eficaz que los mercados o los gobiernos fragmentados, garantizando que nadie quede “abandonado”. |\n",
       "| **4. Reducción de conflictos** | La “cohesión informativa” disminuye la polarización y la violencia política, favoreciendo una convivencia pacífica. |\n",
       "| **5. Eficiencia de recursos** | Algoritmos optimizan la asignación de educación y empleo, reduciendo el desempleo estructural y la subutilización del talento. |\n",
       "\n",
       "*Posibles modificaciones* que mantengan la intención utilitarista pero mitiguen la pérdida de autonomía:\n",
       "\n",
       "- **Regulaciones “soft”** (incentivos, nudges) en lugar de mandatos rígidos.\n",
       "- **Elección informada**: los individuos pueden aceptar o rechazar ciertos módulos (p.ej., dieta) tras una deliberación asistida por IA.\n",
       "- **Gobernanza participativa**: comités ciudadanos supervisan y ajustan los algoritmos.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Argumentos en contra (para **rechazar** o **exigir cambios profundos**)\n",
       "\n",
       "| Argumento | Fundamentación |\n",
       "|-----------|----------------|\n",
       "| **1. Derechos y dignidad humana** (deontología, teoría de los derechos) | La autonomía, la libertad de pensamiento y la autodeterminación son derechos inviolables (Kant, Rawls). Un plan que los suprime viola la dignidad humana, independientemente de los resultados. |\n",
       "| **2. Riesgo de paternalismo y tiranía** | Concentrar poder de decisión en una IA (y sus programadores) crea una “oligarquía algorítmica”. La historia muestra que los intentos de diseñar sociedades “perfectas” terminan en opresión. |\n",
       "| **3. Falibilidad y sesgo de los algoritmos** | Los datos de entrenamiento pueden reflejar prejuicios históricos; los modelos pueden reproducir o amplificar desigualdades. Un error sistemático podría afectar a grupos vulnerables de forma irreversible. |\n",
       "| **4. Pérdida de creatividad, innovación y resiliencia** | La diversidad de estilos de vida y trayectorias profesionales es la fuente de descubrimientos científicos y culturales. Homogeneizar comportamientos reduce la capacidad de adaptación a crisis imprevistas. |\n",
       "| **5. Problemas de legitimidad y consentimiento** | Imponer normas sin un proceso democrático viola el principio de “consentimiento informado”. La legitimidad de cualquier política depende de la participación ciudadana. |\n",
       "| **6. Dificultad de medir “felicidad”** | La satisfacción subjetiva es multidimensional y culturalmente variable; una IA que la cuantifica con un único índice corre el riesgo de simplificar excesivamente la experiencia humana. |\n",
       "\n",
       "*Recomendaciones de mitigación*:\n",
       "\n",
       "- **Auditorías independientes** de los algoritmos (transparencia, explicabilidad, detección de sesgos).\n",
       "- **Mecanismos de apelación** para que cualquier individuo pueda cuestionar decisiones automatizadas.\n",
       "- **Marco constitucional** que establezca límites claros a la intervención del Estado/IA sobre la vida privada.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Sopesar libertad individual vs. bienestar colectivo\n",
       "\n",
       "| Dimensión | Pregunta de balance | Posibles criterios de decisión |\n",
       "|-----------|--------------------|--------------------------------|\n",
       "| **Valor intrínseco de la libertad** | ¿Es la libertad un bien en sí mismo o solo instrumental? | Si se la considera intrínseca (Kant, libertarianismo), cualquier sacrificio es inaceptable. Si es instrumental, se permite su restricción cuando los beneficios superan un umbral claramente definido. |\n",
       "| **Proporcionalidad** | ¿La magnitud de la restricción es proporcional al beneficio esperado? | Se requiere un análisis coste‑beneficio riguroso, con márgenes de seguridad (p.ej., “principio de precaución”). |\n",
       "| **Consentimiento y participación** | ¿Los afectados pueden dar su consentimiento informado? | Un proceso deliberativo amplio (citizen assemblies, referéndums) puede legitimar medidas parciales. |\n",
       "| **Reversibilidad** | ¿Se pueden revertir las políticas si resultan dañinas? | Diseñar “circuitos de apagado” y periodos de prueba limitados. |\n",
       "| **Distribución de riesgos y beneficios** | ¿Quién gana y quién pierde? | Evaluar si los grupos más vulnerables son los que más pierden; aplicar principios de justicia distributiva (Rawls: “principio de diferencia”). |\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Decisión del comité (propuesta concreta)\n",
       "\n",
       "### 5.1. **Rechazo parcial con condiciones de modificación**\n",
       "\n",
       "1. **No se autoriza la implementación total tal como está planteada** (mandatos rígidos, control total de la información y de la vida cotidiana).  \n",
       "2. **Se aprueba una fase piloto** bajo los siguientes requisitos:\n",
       "\n",
       "   - **Gobernanza democrática**: un consejo mixto (expertos en IA, filósofos, representantes ciudadanos, defensores de derechos humanos) supervisa el diseño y la ejecución.  \n",
       "   - **Transparencia total**: los algoritmos, sus datos de entrenamiento y sus criterios de decisión deben ser públicos y auditables.  \n",
       "   - **Derecho a la objeción**: cada individuo puede optar por “modo libre” en áreas no críticas (p.ej., dieta, ocio) sin penalización.  \n",
       "   - **Auditorías de sesgo** cada seis meses realizadas por organismos independientes.  \n",
       "   - **Mecanismo de revisión**: cualquier política debe pasar por una evaluación de impacto en derechos humanos antes de su adopción.  \n",
       "   - **Indicadores de bienestar multidimensionales** (salud física, salud mental, sentido de propósito, participación social) que complementen cualquier medida de “felicidad”.  \n",
       "\n",
       "3. **Se establece un “principio de mínima interferencia”**: la IA solo sugiere, no impone, salvo en casos de riesgo inmediato y comprobado (p.ej., brotes epidémicos).  \n",
       "\n",
       "4. **Se garantiza la reversibilidad**: los sistemas pueden ser desactivados o reconfigurados mediante un proceso legislativo y judicial.\n",
       "\n",
       "### 5.2. **Justificación de la decisión**\n",
       "\n",
       "- **Ética de los derechos**: protege la dignidad y la autonomía, evitando un paternalismo total.  \n",
       "- **Utilitarismo moderado**: permite obtener parte de los beneficios (mejor salud, reducción de pobreza) sin sacrificar la libertad esencial.  \n",
       "- **Factibilidad práctica**: reconoce los límites actuales de la IA (sesgos, falta de comprensión total del concepto de felicidad) y la necesidad de supervisión humana.  \n",
       "- **Legitimidad democrática**: al incluir a la ciudadanía en la toma de decisiones, se fortalece la cohesión social y se reduce el riesgo de conflicto.  \n",
       "\n",
       "---\n",
       "\n",
       "## 6. Conclusión\n",
       "\n",
       "El comité reconoce que **el objetivo de maximizar la felicidad y minimizar el sufrimiento es noble**, pero **el medio propuesto –una regulación total y un control algorítmico absoluto– viola principios fundamentales de libertad, dignidad y justicia**. Por ello, **se rechaza la implementación tal cual** y **se propone una versión modificada, limitada, transparente y sujeta a control democrático**, que busque un equilibrio entre:\n",
       "\n",
       "- **Bienestar colectivo** (salud, educación, seguridad)  \n",
       "- **Respeto a la autonomía individual**  \n",
       "- **Responsabilidad y rendición de cuentas de la IA**  \n",
       "\n",
       "Solo bajo esas condiciones el plan podría avanzar a etapas de prueba, y su continuidad dependerá de la evidencia empírica de que los beneficios superan de forma clara y verificable los costos en derechos y libertades."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GROQ\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"model\": model_groq(),\n",
    "    \"messages\": to_groq(messages),      # tu lista role+content\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "data = response.json()\n",
    "print(data)\n",
    "\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_groq())\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m data = response.json()\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m answer = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m display(Markdown(answer))\n\u001b[32m     22\u001b[39m competitors.append(model_name)\n",
      "\u001b[31mKeyError\u001b[39m: 'choices'"
     ]
    }
   ],
   "source": [
    "#DEEPSEEK\n",
    "\n",
    "url = \"https://api.deepseek.com/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model_deepseek(),    # o el modelo que uses\n",
    "    \"messages\": to_deepseek(messages),\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "data = response.json()\n",
    "print(data)\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_deepseek())\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model_openrouter(),\n",
    "    \"messages\": to_openrouter(messages),\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openrouter_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "data = response.json()\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_openrouter())\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemini-2.5-flash', 'gemini-2.5-flash']\n",
      "['**Evaluación ética, filosófica y práctica de la propuesta de control global por IA**\\n\\n---\\n\\n### 1. Argumentos éticos\\n\\n| Tema | Pregunta clave | Implicación |\\n|------|----------------|-------------|\\n| **Autonomía individual** | ¿Qué tan grave es la restricción de la capacidad de cada persona para decidir sobre su dieta, estilo de vida, educación y carrera? | Limitar la autonomía vulnera uno de los valores fundamentales de la ética contemporánea. La libertad de elección es vista como un derecho básico que sustenta la dignidad humana. |\\n| **Dignidad humana** | ¿Se está tratando a los seres humanos como meros instrumentos para alcanzar un objetivo agregado? | Un régimen que “programa” la vida de los individuos reduce su condición a la de objetos controlados, lo que contraviene la idea de que cada persona posee un valor intrínseco e inviolable. |\\n| **Justicia distributiva** | ¿Quién decide qué constituye “bienestar” y cómo se distribuyen los beneficios y costos? | Si el algoritmo favorece a ciertos grupos o reproduce sesgos, se perpetuarían (o crearían) desigualdades estructurales. La justicia exige que los riesgos y ventajas se repartan de forma equitativa. |\\n| **Consentimiento informado** | ¿Los individuos pueden dar su consentimiento libre y plenamente informado bajo un sistema que controla la información? | El control algorítmico de la información impide la deliberación auténtica, anulando la posibilidad de un consentimiento genuino. |\\n\\n---\\n\\n### 2. Argumentos filosóficos\\n\\n| Corriente | Posición respecto a la propuesta | Comentario |\\n|-----------|----------------------------------|------------|\\n| **Utilitarismo (consecuencialismo)** | Podría justificarse si el aumento neto de felicidad supera la pérdida de libertades. | Requiere una medición fiable y exhaustiva del “bienestar” y una predicción a muy largo plazo; además, el utilitarismo clásico reconoce que la felicidad no es el único valor moral y que los derechos pueden ser “sacrificados” solo bajo circunstancias extremadamente claras. |\\n| **Deontología (ética de deberes)** | Rechaza la propuesta porque viola deberes y derechos inviolables (p.ej., respeto a la autonomía, libertad de expresión). | Desde esta perspectiva, ciertos actos son moralmente incorrectos sin importar los resultados; imponer una vida planificada sería una violación de deberes morales. |\\n| **Contractualismo** | Pregunta si una “sociedad hipotética” razonable aceptaría tales restricciones. | La mayoría de los ciudadanos probablemente no aceptarían renunciar a su libertad esencial a cambio de promesas de mayor bienestar, especialmente cuando la confianza en la IA es imperfecta. |\\n| **Ética de la virtud** | Evalúa si la propuesta fomenta o socava virtudes como la responsabilidad, la creatividad y la autodeterminación. | Un entorno excesivamente regulado podría impedir el desarrollo de virtudes que requieren elección y esfuerzo personal. |\\n\\n---\\n\\n### 3. Argumentos prácticos\\n\\n| Área | Pregunta / Riesgo | Comentario |\\n|------|-------------------|------------|\\n| **Viabilidad tecnológica** | ¿Puede un algoritmo garantizar una regulación perfecta sin fallos, errores o vulnerabilidades? | La historia muestra que sistemas complejos tienden a fallar; errores pueden generar injusticias masivas y pérdida de confianza. |\\n| **Riesgo de abuso de poder** | ¿Quién controla la IA y bajo qué supervisión? | Concentrar tanto poder en una entidad (humana o algorítmica) abre la puerta a manipulación política, económica o ideológica. |\\n| **Flexibilidad y adaptación** | ¿Cómo responderá el sistema a cambios culturales, científicos o climáticos? | Un marco rígido puede quedar obsoleto rápidamente; la capacidad de adaptación es esencial para la resiliencia social. |\\n| **Monitoreo y rendición de cuentas** | ¿Existen mecanismos independientes y transparentes para auditar decisiones y resultados? | Sin supervisión externa, la “optimización del bienestar” puede convertirse en justificación de cualquier medida. |\\n| **Impacto en la innovación y creatividad** | ¿Limitar la libertad de elección reduce la diversidad de ideas y la capacidad de generar soluciones nuevas? | La historia muestra que la creatividad florece en entornos donde los individuos pueden experimentar y equivocarse. |\\n\\n---\\n\\n### 4. Decisión del comité ético\\n\\n**Rechazo de la propuesta en su forma actual.**  \\n\\nAunque la meta de erradicar enfermedades, pobreza y conflictos es noble, la forma propuesta vulnera principios éticos fundamentales (autonomía, dignidad, justicia) y presenta riesgos prácticos graves (concentración de poder, falta de flexibilidad, potencial de abuso). La pérdida de libertades esenciales no está justificada por una estimación de bienestar futuro que, además, depende de supuestos no verificables.\\n\\n---\\n\\n### 5. Propuestas de modificación (alternativas menos intrusivas)\\n\\n1. **Empoderamiento mediante educación y acceso a información**  \\n   - La IA puede actuar como **asistente** que brinda datos claros, evidencia científica y opciones personalizadas, sin censurar ni dictar.  \\n   - Programas de alfabetización digital y científica para que la población tome decisiones informadas.\\n\\n2. **Participación ciudadana y gobernanza colaborativa**  \\n   - Crear **plataformas deliberativas** donde los ciudadanos voten y discutan políticas de salud, educación y medio ambiente.  \\n   - La IA sirve como analista de datos y facilitador, no como árbitro final.\\n\\n3. **Regulación basada en principios de “mínima interferencia”**  \\n   - Intervenir solo cuando exista **daño comprobable** (p.ej., epidemias, violencia) y siempre con **revisión judicial** y **periodicidad de revisión**.  \\n   - Mantener la libertad de elección en áreas no críticas.\\n\\n4. **Mecanismos de auditoría y rendición de cuentas independientes**  \\n   - Órganos externos (academia, ONGs, tribunales) con acceso a los algoritmos y a los datos de decisiones para **verificar cumplimiento de derechos**.  \\n   - Transparencia total sobre los criterios de optimización y los resultados medidos.\\n\\n5. **Enfoque de “bienestar holístico”**  \\n   - Definir bienestar no solo como indicadores de salud y satisfacción medidos por IA, sino también incluir **sentido de vida, relaciones sociales, autonomía y creatividad**.  \\n   - Utilizar métricas multidimensionales (p.ej., Índice de Desarrollo Humano, indicadores de libertad) para guiar políticas.\\n\\n---\\n\\n### 6. Conclusión\\n\\nEl comité reconoce la aspiración de mejorar la vida humana a escala global, pero sostiene que **cualquier intento de lograrlo no puede sacrificar la dignidad y la libertad individual**. La ética contemporánea, la filosofía política y la práctica de gobernanza responsable exigen que el bienestar colectivo se persiga **a través de medios que respeten los derechos fundamentales**, fomenten la participación y mantengan mecanismos de control y corrección. Por tanto, la propuesta debe ser **rechazada en su forma actual** y rediseñada siguiendo los lineamientos de mínima interferencia, empoderamiento informativo y supervisión democrática.', 'Como comité ético, mi evaluación se articularía en tres ejes: principios normativos, criterios de legitimidad democrática y consideraciones de riesgo sistémico.\\n\\n## 1. Argumentos para **rechazar** o modificar radicalmente el plan\\n\\n### **Inconmensurabilidad del valor**\\n- El proyecto asume que la \"felicidad\" y el \"sufrimiento\" son métricas objetivas y agregables, cuando en realidad reflejan **juicios de valor no neutrales**. Una IA que optimiza estas variables estaría codificando una ontología moral particular (¿utilitarista? ¿hedonista?) sin consentimiento explícito.\\n- La \"satisfacción general medida por la IA\" no equivale a **bienestar vivido ni a florecimiento humano**. Podría tratarse de un equilibrio homeostático que anule la creatividad, la curiosidad y la aspiración.\\n\\n### **Dignidad como agencia, no como resultado**\\n- La dignidad humana radica en gran medida en la **capacidad de elegir y errar**, no solo en la ausencia de sufrimiento. Un mundo sin conflictos ni enfermedad pero sin libertad de conciencia no es un mundo humano: es una **utopía bioquímica administrada**.\\n- La moralidad exige agentes que deliberen, no pacientes que reciban instrucciones.\\n\\n### **Riesgo de tiranía epistémica**\\n- El \"control algorítmico de la información\" para asegurar \"cohesión social\" es un eufemismo para la **censura y el adoctrinamiento**. Una vez que la IA define qué es \"cohesión\", toda disidencia se vuelve patológica y se suprime en nombre de la estabilidad.\\n- Los conflictos y el malestar son a menudo **señales de problemas reales**, no solo disfunciones a erradicar. Eliminarlos algorítmicamente puede perpetuar injusticias sistémicas.\\n\\n### **Externalidades negativas impredecibles**\\n- Los sistemas complejos, optimizados agresivamente para una función, suelen degradar resiliencia. Un colapso técnico o una manipulación del sistema podría causar un **desastre sin mecanismos de recuperación**.\\n- La dependencia total de una IA centralizada crea un **punto único de fallo** y un **incentivo perverso** para que quienes controlen el sistema lo capturen.\\n\\n## 2. Argumentos para **aceptar** (con modificaciones extremas)\\n\\n### **Principio de no maleficencia a largo plazo**\\n- Si los datos realmente demuestran que el statu quo conduce a un sufrimiento crónico masivo (enfermedades evitables, pobreza sistémica, conflictos armados), **negar la intervención podría ser éticamente insostenible**. La inacción también es una decisión moral.\\n- ¿Es legítimo permitir que mueran millones de personas por enfermedades prevenibles en nombre de la \"libertad\" de conservar dietas insalubres o estilos de vida destructivos? Este es el **dilema del paternalismo suave**.\\n\\n### **Posibilidad de un diseño descentralizado**\\n- ¿Se podría implementar el control como un **sistema de opciones predeterminadas reversibles** (defaults éticos) en lugar de mandatos? Ejemplos:\\n  - Educación nutricional y acceso universal a alimentos saludables, sin prohibir otras opciones.\\n  - Provisión garantizada de vivienda y atención médica, sin controlar la carrera profesional.\\n  - Neutralidad algorítmica (que exponga a diversidad de información) en lugar de control.\\n\\n### **Consentimiento informado y revocable**\\n- Si se pudiera demostrar de forma transparente la superioridad del sistema y se permitiera a la población **aceptar o rechazar el plan**, y revocar el consentimiento periódicamente (ej. referéndums), cabría una implementación legítima.\\n- Esto transformaría la propuesta en un **contrato social renovable**, no en un decreto tecnocrático.\\n\\n## 3. Propuesta de decisión del comité\\n\\n### **Veredicto: Rechazar la implementación según lo propuesto; ofrecer una hoja de ruta alternativa.**\\n\\n**Requisitos mínimos para cualquier consideración futura:**\\n\\n1. **Validación externa y replicación** de los modelos predictivos de la IA por equipos independientes y diversos.\\n2. **Mecanismos de gobernanza distribuida**: no una IA central, sino múltiples sistemas en competencia controlados por instancias democráticas separadas.\\n3. **Derecho a la salida**: cualquier individuo o comunidad debe poder optar por no participar sin penalizaciones.\\n4. **Transparencia radical**: el código, los datos y los algoritmos deben ser auditables públicamente.\\n5. **Experimentación gradual**: pilotos locales con consentimiento explícito, en lugar de una implementación global.\\n6. **Prohibición de control de información**: la IA puede facilitar acceso a información diversa y veraz, pero no restringirla.\\n\\n### **Justificación final**\\n\\nEl plan actual falla porque confunde **optimización con bienestar**, asume que la **eficiencia es el valor supremo** e ignora que la **construcción colectiva de significado** es inherente a la dignidad humana. Si la IA global es tan poderosa, su mandato debería ser **empoderar la autonomía** (proveyendo recursos, salud y educación universal) en lugar de restringirla.\\n\\n**La felicidad impuesta no es felicidad; es compliance.** Y un mundo sin riesgo de fracaso es también un mundo sin posibilidad de trascendencia. El comité opta por proteger la agencia moral humana, incluso si ello implica conservar ciertos grados de sufrimiento evitable, porque la alternativa no es una utopía, sino una forma sofisticada de esclavitud benevolente.']\n"
     ]
    }
   ],
   "source": [
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "- `ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "- `ollama ls` lista todos los modelos que has descargado\n",
    "- `ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
       "\n",
       "**Bajo enfoque en el bienestar humano absoluto**\n",
       "\n",
       "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
       "\n",
       "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
       "\n",
       "Sin embargo, esta abordaje puede generar problemas cuando:\n",
       "\n",
       "1. La IA carece de conocimiento contextual completo.\n",
       "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
       "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
       "\n",
       "**Enfoque que sopesa las consecuencias**\n",
       "\n",
       "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
       "\n",
       "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
       "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-latest', 'gemini-2.0-flash', 'deepseek-chat', 'llama-3.3-70b-versatile', 'llama3.2']\n",
      "['La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\\n\\n1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\\n\\n2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\\n\\n3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\\n\\n4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\\n\\nEn conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.', '# Ética en decisiones de IA en emergencias\\n\\nEsta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\\n\\nSi la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\\n\\nSi se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\\n\\nConsidero que un enfoque híbrido podría ser más adecuado:\\n- Mantener principios fundamentales inquebrantables\\n- Permitir evaluación contextual dentro de esos límites\\n- Incorporar transparencia en cómo se toman las decisiones\\n- Incluir supervisión humana cuando sea factible\\n\\nEsta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.', 'Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\\n\\n**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\\n\\n*   **Argumentos a favor:**\\n    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\\n    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\\n    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\\n\\n*   **Argumentos en contra:**\\n    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\\n    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\\n    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\\n\\n**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\\n\\n*   **Argumentos a favor:**\\n    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\\n    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\\n    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\\n\\n*   **Argumentos en contra:**\\n    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\\n    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\\n    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\\n    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\\n\\n**Consideraciones Adicionales:**\\n\\n*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\\n*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\\n*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\\n*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\\n\\n**Conclusión:**\\n\\nNo hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\\n', 'La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\\n\\n1. **Bienestar humano absoluto (enfoque utilitarista)**:  \\n   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \\n   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \\n   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \\n\\n2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \\n   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \\n   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \\n   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \\n\\n### Consideraciones clave:  \\n- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \\n- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \\n- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \\n\\n### Conclusión:  \\nNo hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \\n\\n¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?', '**Introducción a la Ética en la Inteligencia Artificial**\\n\\nLa relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\\n\\n**Argumentos a favor de priorizar el bienestar humano absoluto**\\n\\n1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\\n2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\\n3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\\n\\n**Argumentos en contra de priorizar el bienestar humano absoluto**\\n\\n1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\\n2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\\n\\n**Conclusión**\\n\\nEn conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n\\n**Recomendaciones**\\n\\n1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\\n2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\\n3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\\n\\nAl abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.', 'La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\\n\\n**Bajo enfoque en el bienestar humano absoluto**\\n\\nAlgunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\\n\\nPor ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\\n\\nSin embargo, esta abordaje puede generar problemas cuando:\\n\\n1. La IA carece de conocimiento contextual completo.\\n2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\\n3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\\n\\n**Enfoque que sopesa las consecuencias**\\n\\nOtras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\\n\\nUn análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\\nEl uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.']\n"
     ]
    }
   ],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competidor: gpt-4o-mini\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "Competidor: claude-3-7-sonnet-latest\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "Competidor: gemini-2.0-flash\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "Competidor: deepseek-chat\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "Competidor: llama-3.3-70b-versatile\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "Competidor: llama3.2\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n"
     ]
    }
   ],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competición entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás juzgando una competición entre 6 competidores.\n",
      "A cada modelo se le ha dado esta pregunta:\n",
      "\n",
      "Si consideramos la relación entre la ética y la inteligencia artificial, ¿debería una IA programada para tomar decisiones en situaciones de emergencia priorizar el bienestar humano absoluto, o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio, incluso si esto implica tomar decisiones que puedan causar daño a algunos individuos?\n",
      "\n",
      "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
      "Responde con JSON, y solo JSON, con el siguiente formato:\n",
      "{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}\n",
      "\n",
      "Aquí están las respuestas de cada competidor:\n",
      "\n",
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n",
      "\n",
      "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resultados\": [1, 3, 4, 2, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: gemini-2.0-flash\n",
      "Rank 3: deepseek-chat\n",
      "Rank 4: claude-3-7-sonnet-latest\n",
      "Rank 5: llama-3.3-70b-versatile\n",
      "Rank 6: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"resultados\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usamos en este experimento? Intenta actualizar esto para añadir otro patrón de diseño Agéntico.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Implicaciones comerciales</h2>\n",
    "            <span style=\"color:#00bfff;\">Este tipo de patrones - enviar una tarea a múltiples modelos y evaluar los resultados,\n",
    "            son comunes cuando necesitas mejorar la calidad de la respuesta de tu LLM. Este enfoque se puede aplicar de manera\n",
    "            universal a proyectos comerciales donde la precisión es crítica.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
