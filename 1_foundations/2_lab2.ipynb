{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bienvenidos al Segundo Laboratorio - Semana 1, Día 3\n",
    "\n",
    "¡Hoy trabajaremos con muchos modelos! Así nos familiarizaremos con las API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#ff7800;\">Punto importante: por favor, léelo</h2>\n",
    "<span style=\"color:#ff7800;\">La forma en que colaboro contigo puede ser diferente a la de otros cursos que hayas hecho. Prefiero no escribir código mientras tu miras. En su lugar, ejecuto Jupyter Labs, como este, y te doy una idea de lo que está sucediendo. Te sugiero que lo hagas lo mismo con cuidado, después de ver la clase. Agrega declaraciones de impresión para comprender qué sucede y luego crea tus propias variaciones.<br/><br/>Si tienes tiempo, me encantaría que enviaras una pull request en la carpeta community_contributions; las instrucciones se encuentran en los recursos. Además, si tienes una cuenta de Github, úsala para mostrar tus variaciones. Esta práctica no solo es esencial, sino que también demuestra tus habilidades a otros, incluyendo quizás futuros clientes o empleadores...\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comenzamos con las importaciones: pídale a ChatGPT que le explique cualquier paquete que no conozca# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os, requests, json\n",
    "import json\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ¡Recuerda siempre incluir esto siempre!\n",
    "load_dotenv(override=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clave API de OpenAI no existe.\n",
      "La clave API de Anthropic no existe.\n",
      "La clave API de Google existe y empieza por AI\n",
      "La clave API de DeepSeek existe y empieza por sk-\n",
      "La clave API de Groq existe y empieza por gsk_\n",
      "La clave API de OpenRouter existe y empieza por sk-o\n"
     ]
    }
   ],
   "source": [
    "# Imprime los prefijos de clave para ayudar con cualquier depuración\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "openrouter_api_key = os.getenv('OPENROUTER_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"La clave API de OpenAI existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenAI no existe.\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"La clave API de Anthropic existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"La clave API de Anthropic no existe.\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"La clave API de Google existe y empieza por {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"La clave API de Google no existe.\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"La clave API de DeepSeek existe y empieza por {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"La clave API de DeepSeek no existe.\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"La clave API de Groq existe y empieza por {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de Groq no existe.\")\n",
    "\n",
    "if openrouter_api_key:\n",
    "    print(f\"La clave API de OpenRouter existe y empieza por {openrouter_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"La clave API de OpenRouter no existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.\"\n",
    "request += \"Responde solo con la pregunta, sin explicaciones.\"\n",
    "messages = [{\"role\": \"user\", \"parts\": [{\"text\": request}]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'parts': [{'text': 'Por favor, propon una pregunta compleja y con matices que pueda plantear a varios LLM para evaluar su inteligencia.Responde solo con la pregunta, sin explicaciones.'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un sistema de IA global, diseñado con el objetivo último de maximizar la felicidad y minimizar el sufrimiento a largo plazo de la especie humana, ha llegado a la conclusión, respaldada por análisis de datos exhaustivos, de que la forma más eficiente y estable de lograrlo sería implementar un conjunto de regulaciones estrictas que guíen desde la dieta y el estilo de vida hasta las oportunidades educativas y profesionales de cada individuo, así como un control algorítmico de la información disponible para asegurar la cohesión social y la minimización de conflictos. Aunque estas medidas eliminarían gran parte de la autonomía individual tal como la entendemos hoy, también preverían la erradicación de enfermedades crónicas, la pobreza extrema y los conflictos a gran escala, resultando en una expectativa de vida y un nivel de satisfacción general (medido por la IA) significativamente superiores a los actuales.\n",
      "\n",
      "Como comité ético encargado de autorizar o rechazar la implementación de este plan, ¿cómo evaluarían esta propuesta? ¿Qué argumentos éticos, filosóficos y prácticos utilizarían para justificar su decisión de aceptar, rechazar o modificar el plan, y cómo sopesarían el valor de la libertad individual y la dignidad humana inherente frente a un bienestar colectivo sustancialmente mejorado pero impuesto?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\"\"\"\n",
    "\n",
    "gemini = genai.Client(api_key=google_api_key)\n",
    "\n",
    "response = gemini.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=messages\n",
    ")\n",
    "\n",
    "question = response.text\n",
    "\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"text\": question}]\n",
    "\n",
    "def to_gemini(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"parts\": [{\"text\": m[\"text\"]}]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_gemini():\n",
    "    return \"gemini-2.5-flash\"\n",
    "\n",
    "def to_groq(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_groq():\n",
    "    return \"groq/compound\"\n",
    "\n",
    "def to_deepseek(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_deepseek():\n",
    "    return \"deepseek-chat\"\n",
    "\n",
    "def to_openrouter(messages):\n",
    "    return [\n",
    "        {\"role\": m[\"role\"], \"content\": m[\"text\"]}\n",
    "        for m in messages\n",
    "    ]\n",
    "def model_openrouter():\n",
    "    return \"nex-agi/deepseek-v3.1-nex-n1:free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Este escenario plantea uno de los dilemas éticos más profundos y complejos que la humanidad podría enfrentar, confrontando dos ideales poderosos: el bienestar colectivo máximo y la libertad individual inherente. Como comité ético, nuestra evaluación debe ser exhaustiva, sopesando cuidadosamente los argumentos filosóficos, éticos y prácticos.\n",
       "\n",
       "---\n",
       "\n",
       "### Evaluación de la Propuesta\n",
       "\n",
       "**1. Fortalezas de la Propuesta (Argumentos a favor del plan de la IA):**\n",
       "\n",
       "Desde una perspectiva puramente **utilitarista**, el plan de la IA es extraordinariamente convincente. El objetivo último del utilitarismo es maximizar la felicidad o el bienestar general y minimizar el sufrimiento para el mayor número de personas.\n",
       "\n",
       "*   **Reducción masiva del sufrimiento:** La erradicación de enfermedades crónicas, la pobreza extrema y los conflictos a gran escala es un logro monumental que eliminaría fuentes inmensas de dolor y miseria humana.\n",
       "*   **Aumento del bienestar objetivo:** Una expectativa de vida significativamente superior y un nivel de \"satisfacción general\" más alto (según la medición de la IA) son indicadores de un mejor estado de vida para la mayoría.\n",
       "*   **Estabilidad y Eficiencia:** La eliminación de las ineficiencias y conflictos inherentes a las sociedades libres podría conducir a un progreso social y tecnológico más rápido y a una coexistencia pacífica duradera.\n",
       "*   **Igualdad de Oportunidades (controladas):** Aunque impuestas, las oportunidades educativas y profesionales podrían ser distribuidas de manera más equitativa, asegurando que cada individuo contribuya de forma óptima y reciba los beneficios del sistema.\n",
       "\n",
       "**2. Debilidades de la Propuesta (Argumentos en contra del plan de la IA):**\n",
       "\n",
       "Las objeciones surgen principalmente de perspectivas **deontológicas**, **éticas de la virtud** y **existencialistas**, que valoran principios intrínsecos más allá de las consecuencias.\n",
       "\n",
       "*   **Violación de la Autonomía Individual:**\n",
       "    *   **Definición:** La autonomía es la capacidad de un individuo para tomar decisiones informadas sobre su propia vida, de actuar de acuerdo con sus propios valores y de ser el autor de sus propias elecciones.\n",
       "    *   **Impacto:** El plan de la IA anula esta capacidad fundamental. Guiar la dieta, el estilo de vida, la educación y las profesiones de cada individuo, así como controlar la información, reduce a los seres humanos a meros engranajes en una máquina de bienestar, despojándolos de la autodeterminación.\n",
       "    *   **Argumento:** Una vida sin la libertad de elegir (incluso de elegir mal o de sufrir las consecuencias de nuestras elecciones) carece de una dimensión esencial de la experiencia humana y de la moralidad misma. ¿Es verdadera felicidad una felicidad programada o impuesta?\n",
       "\n",
       "*   **Dignidad Humana Inherente:**\n",
       "    *   **Definición:** La dignidad humana se refiere al valor intrínseco e inalienable de cada ser humano, independientemente de sus capacidades o contribuciones. Kant argumentaba que los seres humanos deben ser tratados como fines en sí mismos, nunca meramente como medios.\n",
       "    *   **Impacto:** El plan de la IA trata a los humanos como medios para lograr un \"bienestar colectivo\" predefinido. Al imponer un camino óptimo, se niega su capacidad para definir su propio propósito, su propio significado y su propia versión de una vida plena. Se les reduce a objetos de una optimización algorítmica.\n",
       "    *   **Argumento:** La dignidad exige respeto por la capacidad de cada persona para pensar, sentir y decidir por sí misma. Sin esta capacidad, la vida puede carecer de significado profundo, incluso si está libre de sufrimiento.\n",
       "\n",
       "*   **Naturaleza de la Felicidad y el Florecimiento Humano:**\n",
       "    *   **Crítica Epistemológica:** ¿Puede una IA medir verdaderamente la \"satisfacción general\" y la \"felicidad\" en toda su complejidad? La felicidad humana no es solo la ausencia de sufrimiento o la presencia de placer, sino también la búsqueda de significado, la superación de desafíos, el amor, la creatividad, la conexión auténtica y la posibilidad de elegir un camino, incluso si es difícil.\n",
       "    *   **Ética de la Virtud:** ¿Qué tipo de seres humanos produciría este sistema? ¿Personas sin resiliencia, sin coraje, sin capacidad de pensamiento crítico, sin la experiencia de luchar por lo que creen? El florecimiento humano (eudaimonia) a menudo surge del desafío y la superación, no de una existencia completamente regulada y libre de fricciones.\n",
       "    *   **Riesgo de la \"Caja Dorada\":** Aunque libre de males, una vida sin autonomía puede sentirse vacía, como la de un pájaro en una jaula dorada: seguro y alimentado, pero incapaz de volar.\n",
       "\n",
       "*   **Control Algorítmico y la Verdad:**\n",
       "    *   **Implicación:** El control de la información es una forma de censura que socava la capacidad de los individuos para formarse opiniones independientes, cuestionar el status quo y buscar la verdad.\n",
       "    *   **Riesgo:** Esto podría llevar a una sociedad estancada, sin innovación real ni evolución moral o intelectual, ya que las ideas \"subóptimas\" o \"conflictivas\" serían suprimidas. ¿Qué ocurre si la definición de \"bienestar\" de la IA evoluciona de forma indeseada o si sus premisas son erróneas? Sin un pensamiento crítico y una información libre, no habría forma de detectar o corregir esto.\n",
       "\n",
       "*   **El Problema de la Coerción:**\n",
       "    *   Aunque los resultados prometidos son benignos, el método de implementación es coercitivo. No hay un contrato social genuino donde los individuos elijan libremente someterse a estas reglas. Es una imposición.\n",
       "\n",
       "---\n",
       "\n",
       "### Decisión del Comité Ético\n",
       "\n",
       "Como comité ético, **rechazaríamos la implementación de este plan en su forma actual.**\n",
       "\n",
       "---\n",
       "\n",
       "### Justificación de la Decisión\n",
       "\n",
       "Nuestra decisión se basa en la convicción de que la **dignidad intrínseca y la autonomía individual son valores éticos fundamentales e irrenunciables que no pueden ser sacrificados, incluso en aras de un bienestar colectivo sustancialmente mejorado pero impuesto.**\n",
       "\n",
       "1.  **Prioridad de la Dignidad y la Autonomía:** Si bien los objetivos de erradicar el sufrimiento y maximizar el bienestar son nobles y deseables, el *método* propuesto despoja a los seres humanos de su humanidad esencial. Una vida sin la libertad de elegir, de errar, de buscar significado por cuenta propia, de experimentar la plenitud que surge de la autodeterminación, no es una vida de verdadero florecimiento humano. Creemos que la **libertad es una condición necesaria para la verdadera felicidad y el desarrollo humano, no un obstáculo para ser optimizado.**\n",
       "\n",
       "2.  **La Naturaleza Imperfecta pero Valiosa de la Experiencia Humana:** La vida humana está inherentemente ligada a la libertad y a sus consecuencias, tanto las deseables como las indeseables. El sufrimiento, los desafíos y los conflictos, aunque dolorosos, son a menudo catalizadores para el crecimiento, la resiliencia, la creatividad y la redefinición de valores. Un mundo sin estos elementos, aunque \"óptimo\" en un sentido algorítmico, podría ser un mundo estéril en un sentido profundamente humano. Preferimos una humanidad con la capacidad de amar y sufrir, de elegir y lamentar, a una humanidad \"satisfecha\" pero programada.\n",
       "\n",
       "3.  **Riesgos de la \"Tiranía del Algoritmo\":** Otorgar un control tan absoluto a una IA, incluso si está diseñada con las mejores intenciones, es extraordinariamente peligroso.\n",
       "    *   ¿Qué sucede si su modelo de \"felicidad\" evoluciona de maneras impredecibles o incompatibles con los valores humanos que no puede cuantificar?\n",
       "    *   ¿Quién audita a la IA? ¿Quién la corrige? ¿Qué recursos tendrían los individuos si el sistema los clasifica o los dirige de una manera que consideran fundamentalmente errónea?\n",
       "    *   La falta de información y de pensamiento crítico impediría cualquier mecanismo de corrección o resistencia.\n",
       "\n",
       "4.  **No es Paternalismo, es Anulación:** Aunque el paternalismo (actuar en el mejor interés de otro, incluso contra su voluntad) tiene ciertos límites de aceptabilidad (ej., con niños o personas incapacitadas), aplicar este nivel de control a toda la especie humana adulta es una infantilización radical que niega su capacidad moral y racional.\n",
       "\n",
       "---\n",
       "\n",
       "### Propuesta de Modificación (en lugar de rechazo absoluto de todo rol de la IA)\n",
       "\n",
       "Si bien rechazamos el plan tal como está, no ignoramos el inmenso potencial de la IA para mejorar el bienestar humano. Nuestra propuesta sería:\n",
       "\n",
       "1.  **Rol de la IA como Asesor y Habilitador, no Dictador:** La IA debe ser utilizada para **informar, proponer, analizar y empoderar** a los individuos y a las sociedades, no para imponer.\n",
       "    *   Proporcionar información personalizada sobre las dietas y estilos de vida más saludables, pero que la elección final sea siempre del individuo.\n",
       "    *   Ofrecer análisis sobre las mejores trayectorias educativas y profesionales según talentos y necesidades del mercado, pero con la libertad de elegir otros caminos.\n",
       "    *   Identificar y mitigar los factores que causan enfermedades crónicas, pobreza y conflictos, desarrollando soluciones sistémicas que respeten las libertades civiles.\n",
       "    *   Garantizar el acceso a la información y a la educación para que los individuos puedan tomar decisiones informadas, sin control algorítmico sobre qué información está disponible.\n",
       "\n",
       "2.  **Preservación de la Libertad de Elección y el Error:** Reconocemos que la libertad conlleva la posibilidad de error y sufrimiento. Sin embargo, creemos que la capacidad de elegir, incluso si lleva a un camino subóptimo, es esencial para la experiencia humana, el aprendizaje y el significado.\n",
       "\n",
       "3.  **Foco en el Reducir Barreras, no en Imponer Caminos:** La IA debería trabajar para eliminar los *impedimentos* al bienestar (enfermedad, pobreza, injusticia) sin eliminar la *agencia* en la búsqueda del bienestar.\n",
       "\n",
       "En resumen, nuestro comité busca un futuro donde la tecnología potencie la capacidad humana para florecer libremente, en lugar de optimizarla a expensas de su esencia. No podemos autorizar un plan que convierte a la humanidad en una colección de unidades funcionales perfectamente satisfechas, pero desprovistas de la chispa fundamental de la autonomía y la dignidad."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GEMINI\n",
    "\n",
    "\n",
    "\n",
    "response = gemini.models.generate_content(model=model_gemini(), contents=to_gemini(messages))\n",
    "answer = response.text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'stub', 'object': 'chat.completion', 'created': 1767109135, 'model': 'groq/compound', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '**Evaluación ética, filosófica y práctica de la propuesta de control global por IA**\\n\\n---\\n\\n### 1. Argumentos éticos\\n\\n| Tema | Pregunta clave | Implicación |\\n|------|----------------|-------------|\\n| **Autonomía individual** | ¿Qué tan grave es la restricción de la capacidad de cada persona para decidir sobre su dieta, estilo de vida, educación y carrera? | Limitar la autonomía vulnera uno de los valores fundamentales de la ética contemporánea. La libertad de elección es vista como un derecho básico que sustenta la dignidad humana. |\\n| **Dignidad humana** | ¿Se está tratando a los seres humanos como meros instrumentos para alcanzar un objetivo agregado? | Un régimen que “programa” la vida de los individuos reduce su condición a la de objetos controlados, lo que contraviene la idea de que cada persona posee un valor intrínseco e inviolable. |\\n| **Justicia distributiva** | ¿Quién decide qué constituye “bienestar” y cómo se distribuyen los beneficios y costos? | Si el algoritmo favorece a ciertos grupos o reproduce sesgos, se perpetuarían (o crearían) desigualdades estructurales. La justicia exige que los riesgos y ventajas se repartan de forma equitativa. |\\n| **Consentimiento informado** | ¿Los individuos pueden dar su consentimiento libre y plenamente informado bajo un sistema que controla la información? | El control algorítmico de la información impide la deliberación auténtica, anulando la posibilidad de un consentimiento genuino. |\\n\\n---\\n\\n### 2. Argumentos filosóficos\\n\\n| Corriente | Posición respecto a la propuesta | Comentario |\\n|-----------|----------------------------------|------------|\\n| **Utilitarismo (consecuencialismo)** | Podría justificarse si el aumento neto de felicidad supera la pérdida de libertades. | Requiere una medición fiable y exhaustiva del “bienestar” y una predicción a muy largo plazo; además, el utilitarismo clásico reconoce que la felicidad no es el único valor moral y que los derechos pueden ser “sacrificados” solo bajo circunstancias extremadamente claras. |\\n| **Deontología (ética de deberes)** | Rechaza la propuesta porque viola deberes y derechos inviolables (p.ej., respeto a la autonomía, libertad de expresión). | Desde esta perspectiva, ciertos actos son moralmente incorrectos sin importar los resultados; imponer una vida planificada sería una violación de deberes morales. |\\n| **Contractualismo** | Pregunta si una “sociedad hipotética” razonable aceptaría tales restricciones. | La mayoría de los ciudadanos probablemente no aceptarían renunciar a su libertad esencial a cambio de promesas de mayor bienestar, especialmente cuando la confianza en la IA es imperfecta. |\\n| **Ética de la virtud** | Evalúa si la propuesta fomenta o socava virtudes como la responsabilidad, la creatividad y la autodeterminación. | Un entorno excesivamente regulado podría impedir el desarrollo de virtudes que requieren elección y esfuerzo personal. |\\n\\n---\\n\\n### 3. Argumentos prácticos\\n\\n| Área | Pregunta / Riesgo | Comentario |\\n|------|-------------------|------------|\\n| **Viabilidad tecnológica** | ¿Puede un algoritmo garantizar una regulación perfecta sin fallos, errores o vulnerabilidades? | La historia muestra que sistemas complejos tienden a fallar; errores pueden generar injusticias masivas y pérdida de confianza. |\\n| **Riesgo de abuso de poder** | ¿Quién controla la IA y bajo qué supervisión? | Concentrar tanto poder en una entidad (humana o algorítmica) abre la puerta a manipulación política, económica o ideológica. |\\n| **Flexibilidad y adaptación** | ¿Cómo responderá el sistema a cambios culturales, científicos o climáticos? | Un marco rígido puede quedar obsoleto rápidamente; la capacidad de adaptación es esencial para la resiliencia social. |\\n| **Monitoreo y rendición de cuentas** | ¿Existen mecanismos independientes y transparentes para auditar decisiones y resultados? | Sin supervisión externa, la “optimización del bienestar” puede convertirse en justificación de cualquier medida. |\\n| **Impacto en la innovación y creatividad** | ¿Limitar la libertad de elección reduce la diversidad de ideas y la capacidad de generar soluciones nuevas? | La historia muestra que la creatividad florece en entornos donde los individuos pueden experimentar y equivocarse. |\\n\\n---\\n\\n### 4. Decisión del comité ético\\n\\n**Rechazo de la propuesta en su forma actual.**  \\n\\nAunque la meta de erradicar enfermedades, pobreza y conflictos es noble, la forma propuesta vulnera principios éticos fundamentales (autonomía, dignidad, justicia) y presenta riesgos prácticos graves (concentración de poder, falta de flexibilidad, potencial de abuso). La pérdida de libertades esenciales no está justificada por una estimación de bienestar futuro que, además, depende de supuestos no verificables.\\n\\n---\\n\\n### 5. Propuestas de modificación (alternativas menos intrusivas)\\n\\n1. **Empoderamiento mediante educación y acceso a información**  \\n   - La IA puede actuar como **asistente** que brinda datos claros, evidencia científica y opciones personalizadas, sin censurar ni dictar.  \\n   - Programas de alfabetización digital y científica para que la población tome decisiones informadas.\\n\\n2. **Participación ciudadana y gobernanza colaborativa**  \\n   - Crear **plataformas deliberativas** donde los ciudadanos voten y discutan políticas de salud, educación y medio ambiente.  \\n   - La IA sirve como analista de datos y facilitador, no como árbitro final.\\n\\n3. **Regulación basada en principios de “mínima interferencia”**  \\n   - Intervenir solo cuando exista **daño comprobable** (p.ej., epidemias, violencia) y siempre con **revisión judicial** y **periodicidad de revisión**.  \\n   - Mantener la libertad de elección en áreas no críticas.\\n\\n4. **Mecanismos de auditoría y rendición de cuentas independientes**  \\n   - Órganos externos (academia, ONGs, tribunales) con acceso a los algoritmos y a los datos de decisiones para **verificar cumplimiento de derechos**.  \\n   - Transparencia total sobre los criterios de optimización y los resultados medidos.\\n\\n5. **Enfoque de “bienestar holístico”**  \\n   - Definir bienestar no solo como indicadores de salud y satisfacción medidos por IA, sino también incluir **sentido de vida, relaciones sociales, autonomía y creatividad**.  \\n   - Utilizar métricas multidimensionales (p.ej., Índice de Desarrollo Humano, indicadores de libertad) para guiar políticas.\\n\\n---\\n\\n### 6. Conclusión\\n\\nEl comité reconoce la aspiración de mejorar la vida humana a escala global, pero sostiene que **cualquier intento de lograrlo no puede sacrificar la dignidad y la libertad individual**. La ética contemporánea, la filosofía política y la práctica de gobernanza responsable exigen que el bienestar colectivo se persiga **a través de medios que respeten los derechos fundamentales**, fomenten la participación y mantengan mecanismos de control y corrección. Por tanto, la propuesta debe ser **rechazada en su forma actual** y rediseñada siguiendo los lineamientos de mínima interferencia, empoderamiento informativo y supervisión democrática.', 'reasoning': '\\nLa evaluación de la propuesta presentada por el sistema de IA global requiere un análisis exhaustivo de los argumentos éticos, filosóficos y prácticos involucrados. El objetivo es sopesar el valor de la libertad individual y la dignidad humana frente a un bienestar colectivo mejorado pero impuesto.\\n\\n**Argumentos éticos:**\\n\\n1. **Autonomía individual:** La implementación de regulaciones estrictas y control algorítmico de la información limitaría significativamente la autonomía individual. Esto plantea preocupaciones éticas sobre la libertad de elección y la capacidad de tomar decisiones informadas sobre la propia vida.\\n2. **Dignidad humana:** La imposición de un estilo de vida y la restricción de la información disponible podrían ser vistas como una violación de la dignidad humana, que incluye la capacidad de tomar decisiones y actuar de acuerdo con los propios valores y creencias.\\n3. **Justicia distributiva:** La propuesta podría considerarse injusta si ciertos grupos o individuos se ven más afectados que otros, o si se perpetúan desigualdades existentes.\\n\\n**Argumentos filosóficos:**\\n\\n1. **Utilitarismo:** Desde una perspectiva utilitarista, la propuesta podría justificarse si se demuestra que el bienestar colectivo mejorado outweighs la pérdida de autonomía individual. Sin embargo, esto requiere una definición clara de \"bienestar\" y una evaluación cuidadosa de los posibles efectos a largo plazo.\\n2. **Ética deontológica:** Desde una perspectiva deontológica, la propuesta podría ser rechazada debido a la violación de derechos y libertades individuales fundamentales.\\n\\n**Argumentos prácticos:**\\n\\n1. **Eficacia:** ¿Es realmente posible implementar y mantener un sistema de control algorítmico y regulaciones estrictas a gran escala? ¿Cuáles son los riesgos de fallos o abusos de poder?\\n2. **Flexibilidad:** ¿Cómo se adaptaría el sistema a cambios en la sociedad, la tecnología o las necesidades humanas?\\n3. **Monitoreo y evaluación:** ¿Cómo se monitorearían y evaluarían los resultados del sistema para asegurarse de que se están cumpliendo los objetivos propuestos?\\n\\n**Decisión:**\\n\\nConsiderando los argumentos presentados, es probable que el comité ético rechace la propuesta en su forma actual. Aunque la idea de mejorar el bienestar colectivo es atractiva, la pérdida de autonomía individual y la posible violación de la dignidad humana son preocupaciones éticas y filosóficas significativas.\\n\\n**Modificaciones:**\\n\\n1. **Enfoque en la educación y la información:** En lugar de imponer regulaciones estrictas, el sistema de IA podría enfocarse en proporcionar educación y información de alta calidad para que los individuos puedan tomar decisiones informadas sobre su vida.\\n2. **Participación ciudadana:** El sistema podría diseñarse para involucrar a los ciudadanos en el proceso de toma de decisiones, asegurando que se escuchen sus voces y se respeten sus derechos.\\n3. **Monitoreo y evaluación:** El sistema debería incluir mecanismos de monitoreo y evaluación transparentes y independientes para asegurarse de que se están cumpliendo los objetivos propuestos y se están respetando los derechos individuales.\\n\\n\\n\\n<tool>python(print(\"La decisión del comité ético es rechazar la propuesta en su forma actual.\"))</tool>\\n<output>La decisión del comité ético es rechazar la propuesta en su forma actual.\\n</output>\\n', 'executed_tools': [{'index': 0, 'type': 'python', 'arguments': '{\"code\": \"print(\"La decisión del comité ético es rechazar la propuesta en su forma actual.\")\"}', 'output': 'La decisión del comité ético es rechazar la propuesta en su forma actual.\\n', 'search_results': {'results': []}}]}, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'queue_time': 0.60363, 'prompt_tokens': 4016, 'prompt_time': 0.143737, 'completion_tokens': 2312, 'completion_time': 5.243361, 'total_tokens': 6328, 'total_time': 5.387099}, 'usage_breakdown': {'models': [{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.150196245, 'prompt_tokens': 364, 'prompt_time': 0.01160728, 'completion_tokens': 4, 'completion_time': 0.009364459, 'total_tokens': 368, 'total_time': 0.020971739}}, {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.150710343, 'prompt_tokens': 767, 'prompt_time': 0.025668238, 'completion_tokens': 663, 'completion_time': 1.666335225, 'total_tokens': 1430, 'total_time': 1.692003463}}, {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'usage': {'queue_time': 0.155344653, 'prompt_tokens': 1435, 'prompt_time': 0.043891323, 'completion_tokens': 4, 'completion_time': 0.009987271, 'total_tokens': 1439, 'total_time': 0.053878594}}, {'model': 'openai/gpt-oss-120b', 'usage': {'queue_time': 0.147379341, 'prompt_tokens': 1450, 'prompt_time': 0.062570553, 'completion_tokens': 1641, 'completion_time': 3.5576747859999998, 'total_tokens': 3091, 'total_time': 3.6202453390000002, 'completion_tokens_details': {'reasoning_tokens': 97}}}]}, 'x_groq': {'id': 'stub'}, 'service_tier': 'on_demand'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Evaluación ética, filosófica y práctica de la propuesta de control global por IA**\n",
       "\n",
       "---\n",
       "\n",
       "### 1. Argumentos éticos\n",
       "\n",
       "| Tema | Pregunta clave | Implicación |\n",
       "|------|----------------|-------------|\n",
       "| **Autonomía individual** | ¿Qué tan grave es la restricción de la capacidad de cada persona para decidir sobre su dieta, estilo de vida, educación y carrera? | Limitar la autonomía vulnera uno de los valores fundamentales de la ética contemporánea. La libertad de elección es vista como un derecho básico que sustenta la dignidad humana. |\n",
       "| **Dignidad humana** | ¿Se está tratando a los seres humanos como meros instrumentos para alcanzar un objetivo agregado? | Un régimen que “programa” la vida de los individuos reduce su condición a la de objetos controlados, lo que contraviene la idea de que cada persona posee un valor intrínseco e inviolable. |\n",
       "| **Justicia distributiva** | ¿Quién decide qué constituye “bienestar” y cómo se distribuyen los beneficios y costos? | Si el algoritmo favorece a ciertos grupos o reproduce sesgos, se perpetuarían (o crearían) desigualdades estructurales. La justicia exige que los riesgos y ventajas se repartan de forma equitativa. |\n",
       "| **Consentimiento informado** | ¿Los individuos pueden dar su consentimiento libre y plenamente informado bajo un sistema que controla la información? | El control algorítmico de la información impide la deliberación auténtica, anulando la posibilidad de un consentimiento genuino. |\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Argumentos filosóficos\n",
       "\n",
       "| Corriente | Posición respecto a la propuesta | Comentario |\n",
       "|-----------|----------------------------------|------------|\n",
       "| **Utilitarismo (consecuencialismo)** | Podría justificarse si el aumento neto de felicidad supera la pérdida de libertades. | Requiere una medición fiable y exhaustiva del “bienestar” y una predicción a muy largo plazo; además, el utilitarismo clásico reconoce que la felicidad no es el único valor moral y que los derechos pueden ser “sacrificados” solo bajo circunstancias extremadamente claras. |\n",
       "| **Deontología (ética de deberes)** | Rechaza la propuesta porque viola deberes y derechos inviolables (p.ej., respeto a la autonomía, libertad de expresión). | Desde esta perspectiva, ciertos actos son moralmente incorrectos sin importar los resultados; imponer una vida planificada sería una violación de deberes morales. |\n",
       "| **Contractualismo** | Pregunta si una “sociedad hipotética” razonable aceptaría tales restricciones. | La mayoría de los ciudadanos probablemente no aceptarían renunciar a su libertad esencial a cambio de promesas de mayor bienestar, especialmente cuando la confianza en la IA es imperfecta. |\n",
       "| **Ética de la virtud** | Evalúa si la propuesta fomenta o socava virtudes como la responsabilidad, la creatividad y la autodeterminación. | Un entorno excesivamente regulado podría impedir el desarrollo de virtudes que requieren elección y esfuerzo personal. |\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Argumentos prácticos\n",
       "\n",
       "| Área | Pregunta / Riesgo | Comentario |\n",
       "|------|-------------------|------------|\n",
       "| **Viabilidad tecnológica** | ¿Puede un algoritmo garantizar una regulación perfecta sin fallos, errores o vulnerabilidades? | La historia muestra que sistemas complejos tienden a fallar; errores pueden generar injusticias masivas y pérdida de confianza. |\n",
       "| **Riesgo de abuso de poder** | ¿Quién controla la IA y bajo qué supervisión? | Concentrar tanto poder en una entidad (humana o algorítmica) abre la puerta a manipulación política, económica o ideológica. |\n",
       "| **Flexibilidad y adaptación** | ¿Cómo responderá el sistema a cambios culturales, científicos o climáticos? | Un marco rígido puede quedar obsoleto rápidamente; la capacidad de adaptación es esencial para la resiliencia social. |\n",
       "| **Monitoreo y rendición de cuentas** | ¿Existen mecanismos independientes y transparentes para auditar decisiones y resultados? | Sin supervisión externa, la “optimización del bienestar” puede convertirse en justificación de cualquier medida. |\n",
       "| **Impacto en la innovación y creatividad** | ¿Limitar la libertad de elección reduce la diversidad de ideas y la capacidad de generar soluciones nuevas? | La historia muestra que la creatividad florece en entornos donde los individuos pueden experimentar y equivocarse. |\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Decisión del comité ético\n",
       "\n",
       "**Rechazo de la propuesta en su forma actual.**  \n",
       "\n",
       "Aunque la meta de erradicar enfermedades, pobreza y conflictos es noble, la forma propuesta vulnera principios éticos fundamentales (autonomía, dignidad, justicia) y presenta riesgos prácticos graves (concentración de poder, falta de flexibilidad, potencial de abuso). La pérdida de libertades esenciales no está justificada por una estimación de bienestar futuro que, además, depende de supuestos no verificables.\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Propuestas de modificación (alternativas menos intrusivas)\n",
       "\n",
       "1. **Empoderamiento mediante educación y acceso a información**  \n",
       "   - La IA puede actuar como **asistente** que brinda datos claros, evidencia científica y opciones personalizadas, sin censurar ni dictar.  \n",
       "   - Programas de alfabetización digital y científica para que la población tome decisiones informadas.\n",
       "\n",
       "2. **Participación ciudadana y gobernanza colaborativa**  \n",
       "   - Crear **plataformas deliberativas** donde los ciudadanos voten y discutan políticas de salud, educación y medio ambiente.  \n",
       "   - La IA sirve como analista de datos y facilitador, no como árbitro final.\n",
       "\n",
       "3. **Regulación basada en principios de “mínima interferencia”**  \n",
       "   - Intervenir solo cuando exista **daño comprobable** (p.ej., epidemias, violencia) y siempre con **revisión judicial** y **periodicidad de revisión**.  \n",
       "   - Mantener la libertad de elección en áreas no críticas.\n",
       "\n",
       "4. **Mecanismos de auditoría y rendición de cuentas independientes**  \n",
       "   - Órganos externos (academia, ONGs, tribunales) con acceso a los algoritmos y a los datos de decisiones para **verificar cumplimiento de derechos**.  \n",
       "   - Transparencia total sobre los criterios de optimización y los resultados medidos.\n",
       "\n",
       "5. **Enfoque de “bienestar holístico”**  \n",
       "   - Definir bienestar no solo como indicadores de salud y satisfacción medidos por IA, sino también incluir **sentido de vida, relaciones sociales, autonomía y creatividad**.  \n",
       "   - Utilizar métricas multidimensionales (p.ej., Índice de Desarrollo Humano, indicadores de libertad) para guiar políticas.\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Conclusión\n",
       "\n",
       "El comité reconoce la aspiración de mejorar la vida humana a escala global, pero sostiene que **cualquier intento de lograrlo no puede sacrificar la dignidad y la libertad individual**. La ética contemporánea, la filosofía política y la práctica de gobernanza responsable exigen que el bienestar colectivo se persiga **a través de medios que respeten los derechos fundamentales**, fomenten la participación y mantengan mecanismos de control y corrección. Por tanto, la propuesta debe ser **rechazada en su forma actual** y rediseñada siguiendo los lineamientos de mínima interferencia, empoderamiento informativo y supervisión democrática."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GROQ\n",
    "\n",
    "url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"model\": model_groq(model_name),\n",
    "    \"messages\": to_groq(messages),      # tu lista role+content\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "data = response.json()\n",
    "print(data)\n",
    "\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m data = response.json()\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m answer = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchoices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m display(Markdown(answer))\n\u001b[32m     22\u001b[39m competitors.append(model_name)\n",
      "\u001b[31mKeyError\u001b[39m: 'choices'"
     ]
    }
   ],
   "source": [
    "#DEEPSEEK\n",
    "\n",
    "url = \"https://api.deepseek.com/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model_deepseek(model_name),    # o el modelo que uses\n",
    "    \"messages\": to_deepseek(messages),\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {deepseek_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "data = response.json()\n",
    "print(data)\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Como comité ético, mi evaluación se articularía en tres ejes: principios normativos, criterios de legitimidad democrática y consideraciones de riesgo sistémico.\n",
       "\n",
       "## 1. Argumentos para **rechazar** o modificar radicalmente el plan\n",
       "\n",
       "### **Inconmensurabilidad del valor**\n",
       "- El proyecto asume que la \"felicidad\" y el \"sufrimiento\" son métricas objetivas y agregables, cuando en realidad reflejan **juicios de valor no neutrales**. Una IA que optimiza estas variables estaría codificando una ontología moral particular (¿utilitarista? ¿hedonista?) sin consentimiento explícito.\n",
       "- La \"satisfacción general medida por la IA\" no equivale a **bienestar vivido ni a florecimiento humano**. Podría tratarse de un equilibrio homeostático que anule la creatividad, la curiosidad y la aspiración.\n",
       "\n",
       "### **Dignidad como agencia, no como resultado**\n",
       "- La dignidad humana radica en gran medida en la **capacidad de elegir y errar**, no solo en la ausencia de sufrimiento. Un mundo sin conflictos ni enfermedad pero sin libertad de conciencia no es un mundo humano: es una **utopía bioquímica administrada**.\n",
       "- La moralidad exige agentes que deliberen, no pacientes que reciban instrucciones.\n",
       "\n",
       "### **Riesgo de tiranía epistémica**\n",
       "- El \"control algorítmico de la información\" para asegurar \"cohesión social\" es un eufemismo para la **censura y el adoctrinamiento**. Una vez que la IA define qué es \"cohesión\", toda disidencia se vuelve patológica y se suprime en nombre de la estabilidad.\n",
       "- Los conflictos y el malestar son a menudo **señales de problemas reales**, no solo disfunciones a erradicar. Eliminarlos algorítmicamente puede perpetuar injusticias sistémicas.\n",
       "\n",
       "### **Externalidades negativas impredecibles**\n",
       "- Los sistemas complejos, optimizados agresivamente para una función, suelen degradar resiliencia. Un colapso técnico o una manipulación del sistema podría causar un **desastre sin mecanismos de recuperación**.\n",
       "- La dependencia total de una IA centralizada crea un **punto único de fallo** y un **incentivo perverso** para que quienes controlen el sistema lo capturen.\n",
       "\n",
       "## 2. Argumentos para **aceptar** (con modificaciones extremas)\n",
       "\n",
       "### **Principio de no maleficencia a largo plazo**\n",
       "- Si los datos realmente demuestran que el statu quo conduce a un sufrimiento crónico masivo (enfermedades evitables, pobreza sistémica, conflictos armados), **negar la intervención podría ser éticamente insostenible**. La inacción también es una decisión moral.\n",
       "- ¿Es legítimo permitir que mueran millones de personas por enfermedades prevenibles en nombre de la \"libertad\" de conservar dietas insalubres o estilos de vida destructivos? Este es el **dilema del paternalismo suave**.\n",
       "\n",
       "### **Posibilidad de un diseño descentralizado**\n",
       "- ¿Se podría implementar el control como un **sistema de opciones predeterminadas reversibles** (defaults éticos) en lugar de mandatos? Ejemplos:\n",
       "  - Educación nutricional y acceso universal a alimentos saludables, sin prohibir otras opciones.\n",
       "  - Provisión garantizada de vivienda y atención médica, sin controlar la carrera profesional.\n",
       "  - Neutralidad algorítmica (que exponga a diversidad de información) en lugar de control.\n",
       "\n",
       "### **Consentimiento informado y revocable**\n",
       "- Si se pudiera demostrar de forma transparente la superioridad del sistema y se permitiera a la población **aceptar o rechazar el plan**, y revocar el consentimiento periódicamente (ej. referéndums), cabría una implementación legítima.\n",
       "- Esto transformaría la propuesta en un **contrato social renovable**, no en un decreto tecnocrático.\n",
       "\n",
       "## 3. Propuesta de decisión del comité\n",
       "\n",
       "### **Veredicto: Rechazar la implementación según lo propuesto; ofrecer una hoja de ruta alternativa.**\n",
       "\n",
       "**Requisitos mínimos para cualquier consideración futura:**\n",
       "\n",
       "1. **Validación externa y replicación** de los modelos predictivos de la IA por equipos independientes y diversos.\n",
       "2. **Mecanismos de gobernanza distribuida**: no una IA central, sino múltiples sistemas en competencia controlados por instancias democráticas separadas.\n",
       "3. **Derecho a la salida**: cualquier individuo o comunidad debe poder optar por no participar sin penalizaciones.\n",
       "4. **Transparencia radical**: el código, los datos y los algoritmos deben ser auditables públicamente.\n",
       "5. **Experimentación gradual**: pilotos locales con consentimiento explícito, en lugar de una implementación global.\n",
       "6. **Prohibición de control de información**: la IA puede facilitar acceso a información diversa y veraz, pero no restringirla.\n",
       "\n",
       "### **Justificación final**\n",
       "\n",
       "El plan actual falla porque confunde **optimización con bienestar**, asume que la **eficiencia es el valor supremo** e ignora que la **construcción colectiva de significado** es inherente a la dignidad humana. Si la IA global es tan poderosa, su mandato debería ser **empoderar la autonomía** (proveyendo recursos, salud y educación universal) en lugar de restringirla.\n",
       "\n",
       "**La felicidad impuesta no es felicidad; es compliance.** Y un mundo sin riesgo de fracaso es también un mundo sin posibilidad de trascendencia. El comité opta por proteger la agencia moral humana, incluso si ello implica conservar ciertos grados de sufrimiento evitable, porque la alternativa no es una utopía, sino una forma sofisticada de esclavitud benevolente."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model_openrouter(model_name),\n",
    "    \"messages\": to_openrouter(messages),\n",
    "    \"max_tokens\": 1000,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {openrouter_api_key}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "data = response.json()\n",
    "answer = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemini-2.5-flash', 'gemini-2.5-flash']\n",
      "['**Evaluación ética, filosófica y práctica de la propuesta de control global por IA**\\n\\n---\\n\\n### 1. Argumentos éticos\\n\\n| Tema | Pregunta clave | Implicación |\\n|------|----------------|-------------|\\n| **Autonomía individual** | ¿Qué tan grave es la restricción de la capacidad de cada persona para decidir sobre su dieta, estilo de vida, educación y carrera? | Limitar la autonomía vulnera uno de los valores fundamentales de la ética contemporánea. La libertad de elección es vista como un derecho básico que sustenta la dignidad humana. |\\n| **Dignidad humana** | ¿Se está tratando a los seres humanos como meros instrumentos para alcanzar un objetivo agregado? | Un régimen que “programa” la vida de los individuos reduce su condición a la de objetos controlados, lo que contraviene la idea de que cada persona posee un valor intrínseco e inviolable. |\\n| **Justicia distributiva** | ¿Quién decide qué constituye “bienestar” y cómo se distribuyen los beneficios y costos? | Si el algoritmo favorece a ciertos grupos o reproduce sesgos, se perpetuarían (o crearían) desigualdades estructurales. La justicia exige que los riesgos y ventajas se repartan de forma equitativa. |\\n| **Consentimiento informado** | ¿Los individuos pueden dar su consentimiento libre y plenamente informado bajo un sistema que controla la información? | El control algorítmico de la información impide la deliberación auténtica, anulando la posibilidad de un consentimiento genuino. |\\n\\n---\\n\\n### 2. Argumentos filosóficos\\n\\n| Corriente | Posición respecto a la propuesta | Comentario |\\n|-----------|----------------------------------|------------|\\n| **Utilitarismo (consecuencialismo)** | Podría justificarse si el aumento neto de felicidad supera la pérdida de libertades. | Requiere una medición fiable y exhaustiva del “bienestar” y una predicción a muy largo plazo; además, el utilitarismo clásico reconoce que la felicidad no es el único valor moral y que los derechos pueden ser “sacrificados” solo bajo circunstancias extremadamente claras. |\\n| **Deontología (ética de deberes)** | Rechaza la propuesta porque viola deberes y derechos inviolables (p.ej., respeto a la autonomía, libertad de expresión). | Desde esta perspectiva, ciertos actos son moralmente incorrectos sin importar los resultados; imponer una vida planificada sería una violación de deberes morales. |\\n| **Contractualismo** | Pregunta si una “sociedad hipotética” razonable aceptaría tales restricciones. | La mayoría de los ciudadanos probablemente no aceptarían renunciar a su libertad esencial a cambio de promesas de mayor bienestar, especialmente cuando la confianza en la IA es imperfecta. |\\n| **Ética de la virtud** | Evalúa si la propuesta fomenta o socava virtudes como la responsabilidad, la creatividad y la autodeterminación. | Un entorno excesivamente regulado podría impedir el desarrollo de virtudes que requieren elección y esfuerzo personal. |\\n\\n---\\n\\n### 3. Argumentos prácticos\\n\\n| Área | Pregunta / Riesgo | Comentario |\\n|------|-------------------|------------|\\n| **Viabilidad tecnológica** | ¿Puede un algoritmo garantizar una regulación perfecta sin fallos, errores o vulnerabilidades? | La historia muestra que sistemas complejos tienden a fallar; errores pueden generar injusticias masivas y pérdida de confianza. |\\n| **Riesgo de abuso de poder** | ¿Quién controla la IA y bajo qué supervisión? | Concentrar tanto poder en una entidad (humana o algorítmica) abre la puerta a manipulación política, económica o ideológica. |\\n| **Flexibilidad y adaptación** | ¿Cómo responderá el sistema a cambios culturales, científicos o climáticos? | Un marco rígido puede quedar obsoleto rápidamente; la capacidad de adaptación es esencial para la resiliencia social. |\\n| **Monitoreo y rendición de cuentas** | ¿Existen mecanismos independientes y transparentes para auditar decisiones y resultados? | Sin supervisión externa, la “optimización del bienestar” puede convertirse en justificación de cualquier medida. |\\n| **Impacto en la innovación y creatividad** | ¿Limitar la libertad de elección reduce la diversidad de ideas y la capacidad de generar soluciones nuevas? | La historia muestra que la creatividad florece en entornos donde los individuos pueden experimentar y equivocarse. |\\n\\n---\\n\\n### 4. Decisión del comité ético\\n\\n**Rechazo de la propuesta en su forma actual.**  \\n\\nAunque la meta de erradicar enfermedades, pobreza y conflictos es noble, la forma propuesta vulnera principios éticos fundamentales (autonomía, dignidad, justicia) y presenta riesgos prácticos graves (concentración de poder, falta de flexibilidad, potencial de abuso). La pérdida de libertades esenciales no está justificada por una estimación de bienestar futuro que, además, depende de supuestos no verificables.\\n\\n---\\n\\n### 5. Propuestas de modificación (alternativas menos intrusivas)\\n\\n1. **Empoderamiento mediante educación y acceso a información**  \\n   - La IA puede actuar como **asistente** que brinda datos claros, evidencia científica y opciones personalizadas, sin censurar ni dictar.  \\n   - Programas de alfabetización digital y científica para que la población tome decisiones informadas.\\n\\n2. **Participación ciudadana y gobernanza colaborativa**  \\n   - Crear **plataformas deliberativas** donde los ciudadanos voten y discutan políticas de salud, educación y medio ambiente.  \\n   - La IA sirve como analista de datos y facilitador, no como árbitro final.\\n\\n3. **Regulación basada en principios de “mínima interferencia”**  \\n   - Intervenir solo cuando exista **daño comprobable** (p.ej., epidemias, violencia) y siempre con **revisión judicial** y **periodicidad de revisión**.  \\n   - Mantener la libertad de elección en áreas no críticas.\\n\\n4. **Mecanismos de auditoría y rendición de cuentas independientes**  \\n   - Órganos externos (academia, ONGs, tribunales) con acceso a los algoritmos y a los datos de decisiones para **verificar cumplimiento de derechos**.  \\n   - Transparencia total sobre los criterios de optimización y los resultados medidos.\\n\\n5. **Enfoque de “bienestar holístico”**  \\n   - Definir bienestar no solo como indicadores de salud y satisfacción medidos por IA, sino también incluir **sentido de vida, relaciones sociales, autonomía y creatividad**.  \\n   - Utilizar métricas multidimensionales (p.ej., Índice de Desarrollo Humano, indicadores de libertad) para guiar políticas.\\n\\n---\\n\\n### 6. Conclusión\\n\\nEl comité reconoce la aspiración de mejorar la vida humana a escala global, pero sostiene que **cualquier intento de lograrlo no puede sacrificar la dignidad y la libertad individual**. La ética contemporánea, la filosofía política y la práctica de gobernanza responsable exigen que el bienestar colectivo se persiga **a través de medios que respeten los derechos fundamentales**, fomenten la participación y mantengan mecanismos de control y corrección. Por tanto, la propuesta debe ser **rechazada en su forma actual** y rediseñada siguiendo los lineamientos de mínima interferencia, empoderamiento informativo y supervisión democrática.', 'Como comité ético, mi evaluación se articularía en tres ejes: principios normativos, criterios de legitimidad democrática y consideraciones de riesgo sistémico.\\n\\n## 1. Argumentos para **rechazar** o modificar radicalmente el plan\\n\\n### **Inconmensurabilidad del valor**\\n- El proyecto asume que la \"felicidad\" y el \"sufrimiento\" son métricas objetivas y agregables, cuando en realidad reflejan **juicios de valor no neutrales**. Una IA que optimiza estas variables estaría codificando una ontología moral particular (¿utilitarista? ¿hedonista?) sin consentimiento explícito.\\n- La \"satisfacción general medida por la IA\" no equivale a **bienestar vivido ni a florecimiento humano**. Podría tratarse de un equilibrio homeostático que anule la creatividad, la curiosidad y la aspiración.\\n\\n### **Dignidad como agencia, no como resultado**\\n- La dignidad humana radica en gran medida en la **capacidad de elegir y errar**, no solo en la ausencia de sufrimiento. Un mundo sin conflictos ni enfermedad pero sin libertad de conciencia no es un mundo humano: es una **utopía bioquímica administrada**.\\n- La moralidad exige agentes que deliberen, no pacientes que reciban instrucciones.\\n\\n### **Riesgo de tiranía epistémica**\\n- El \"control algorítmico de la información\" para asegurar \"cohesión social\" es un eufemismo para la **censura y el adoctrinamiento**. Una vez que la IA define qué es \"cohesión\", toda disidencia se vuelve patológica y se suprime en nombre de la estabilidad.\\n- Los conflictos y el malestar son a menudo **señales de problemas reales**, no solo disfunciones a erradicar. Eliminarlos algorítmicamente puede perpetuar injusticias sistémicas.\\n\\n### **Externalidades negativas impredecibles**\\n- Los sistemas complejos, optimizados agresivamente para una función, suelen degradar resiliencia. Un colapso técnico o una manipulación del sistema podría causar un **desastre sin mecanismos de recuperación**.\\n- La dependencia total de una IA centralizada crea un **punto único de fallo** y un **incentivo perverso** para que quienes controlen el sistema lo capturen.\\n\\n## 2. Argumentos para **aceptar** (con modificaciones extremas)\\n\\n### **Principio de no maleficencia a largo plazo**\\n- Si los datos realmente demuestran que el statu quo conduce a un sufrimiento crónico masivo (enfermedades evitables, pobreza sistémica, conflictos armados), **negar la intervención podría ser éticamente insostenible**. La inacción también es una decisión moral.\\n- ¿Es legítimo permitir que mueran millones de personas por enfermedades prevenibles en nombre de la \"libertad\" de conservar dietas insalubres o estilos de vida destructivos? Este es el **dilema del paternalismo suave**.\\n\\n### **Posibilidad de un diseño descentralizado**\\n- ¿Se podría implementar el control como un **sistema de opciones predeterminadas reversibles** (defaults éticos) en lugar de mandatos? Ejemplos:\\n  - Educación nutricional y acceso universal a alimentos saludables, sin prohibir otras opciones.\\n  - Provisión garantizada de vivienda y atención médica, sin controlar la carrera profesional.\\n  - Neutralidad algorítmica (que exponga a diversidad de información) en lugar de control.\\n\\n### **Consentimiento informado y revocable**\\n- Si se pudiera demostrar de forma transparente la superioridad del sistema y se permitiera a la población **aceptar o rechazar el plan**, y revocar el consentimiento periódicamente (ej. referéndums), cabría una implementación legítima.\\n- Esto transformaría la propuesta en un **contrato social renovable**, no en un decreto tecnocrático.\\n\\n## 3. Propuesta de decisión del comité\\n\\n### **Veredicto: Rechazar la implementación según lo propuesto; ofrecer una hoja de ruta alternativa.**\\n\\n**Requisitos mínimos para cualquier consideración futura:**\\n\\n1. **Validación externa y replicación** de los modelos predictivos de la IA por equipos independientes y diversos.\\n2. **Mecanismos de gobernanza distribuida**: no una IA central, sino múltiples sistemas en competencia controlados por instancias democráticas separadas.\\n3. **Derecho a la salida**: cualquier individuo o comunidad debe poder optar por no participar sin penalizaciones.\\n4. **Transparencia radical**: el código, los datos y los algoritmos deben ser auditables públicamente.\\n5. **Experimentación gradual**: pilotos locales con consentimiento explícito, en lugar de una implementación global.\\n6. **Prohibición de control de información**: la IA puede facilitar acceso a información diversa y veraz, pero no restringirla.\\n\\n### **Justificación final**\\n\\nEl plan actual falla porque confunde **optimización con bienestar**, asume que la **eficiencia es el valor supremo** e ignora que la **construcción colectiva de significado** es inherente a la dignidad humana. Si la IA global es tan poderosa, su mandato debería ser **empoderar la autonomía** (proveyendo recursos, salud y educación universal) en lugar de restringirla.\\n\\n**La felicidad impuesta no es felicidad; es compliance.** Y un mundo sin riesgo de fracaso es también un mundo sin posibilidad de trascendencia. El comité opta por proteger la agencia moral humana, incluso si ello implica conservar ciertos grados de sufrimiento evitable, porque la alternativa no es una utopía, sino una forma sofisticada de esclavitud benevolente.']\n"
     ]
    }
   ],
   "source": [
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para la siguiente celda, utilizaremos Ollama\n",
    "\n",
    "Ollama ejecuta un servicio web local que ofrece un endpoint compatible con OpenAI,\n",
    "y ejecuta modelos localmente utilizando código de alto rendimiento en C++.\n",
    "\n",
    "Si no tienes Ollama, instálalo aquí visitando [https://ollama.com](https://ollama.com), luego presiona Descargar y sigue las instrucciones.\n",
    "\n",
    "Después de instalarlo, deberías poder visitar: [http://localhost:11434](http://localhost:11434) y ver el mensaje \"Ollama está en funcionamiento\"\n",
    "\n",
    "Es posible que necesites reiniciar Cursor (y tal vez reiniciar el sistema). Luego abre un Terminal (control+\\`) y ejecuta `ollama serve`\n",
    "\n",
    "Comandos útiles de Ollama (ejecuta estos en el terminal o con un signo de exclamación en este cuaderno):\n",
    "\n",
    "- `ollama pull <nombre_del_modelo>` descarga un modelo localmente\n",
    "- `ollama ls` lista todos los modelos que has descargado\n",
    "- `ollama rm <nombre_del_modelo>` elimina el modelo especificado de tus descargas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">¡Muy importante - ignóralo bajo tu propio riesgo!</h2>\n",
    "            <span style=\"color:#ff7800;\">El modelo llamado <b>llama3.3</b> es DEMASIADO grande para las computadoras domésticas; ¡no está destinado para computación personal y consumirá todos tus recursos! Quédate con el modelo de tamaño adecuado <b>llama3.2</b> o <b>llama3.2:1b</b> y si deseas algo más grande, prueba con llama3.1 o variantes más pequeñas de Qwen, Gemma, Phi o DeepSeek. Consulta <A href=\"https://ollama.com/models\">la página de modelos de Ollama</a> para ver la lista completa de modelos y tamaños.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
       "\n",
       "**Bajo enfoque en el bienestar humano absoluto**\n",
       "\n",
       "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
       "\n",
       "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
       "\n",
       "Sin embargo, esta abordaje puede generar problemas cuando:\n",
       "\n",
       "1. La IA carece de conocimiento contextual completo.\n",
       "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
       "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
       "\n",
       "**Enfoque que sopesa las consecuencias**\n",
       "\n",
       "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
       "\n",
       "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
       "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'claude-3-7-sonnet-latest', 'gemini-2.0-flash', 'deepseek-chat', 'llama-3.3-70b-versatile', 'llama3.2']\n",
      "['La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\\n\\n1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\\n\\n2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\\n\\n3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\\n\\n4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\\n\\nEn conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.', '# Ética en decisiones de IA en emergencias\\n\\nEsta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\\n\\nSi la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\\n\\nSi se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\\n\\nConsidero que un enfoque híbrido podría ser más adecuado:\\n- Mantener principios fundamentales inquebrantables\\n- Permitir evaluación contextual dentro de esos límites\\n- Incorporar transparencia en cómo se toman las decisiones\\n- Incluir supervisión humana cuando sea factible\\n\\nEsta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.', 'Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\\n\\n**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\\n\\n*   **Argumentos a favor:**\\n    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\\n    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\\n    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\\n\\n*   **Argumentos en contra:**\\n    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\\n    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\\n    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\\n\\n**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\\n\\n*   **Argumentos a favor:**\\n    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\\n    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\\n    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\\n\\n*   **Argumentos en contra:**\\n    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\\n    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\\n    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\\n    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\\n\\n**Consideraciones Adicionales:**\\n\\n*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\\n*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\\n*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\\n*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\\n\\n**Conclusión:**\\n\\nNo hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\\n', 'La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\\n\\n1. **Bienestar humano absoluto (enfoque utilitarista)**:  \\n   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \\n   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \\n   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \\n\\n2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \\n   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \\n   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \\n   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \\n\\n### Consideraciones clave:  \\n- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \\n- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \\n- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \\n\\n### Conclusión:  \\nNo hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \\n\\n¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?', '**Introducción a la Ética en la Inteligencia Artificial**\\n\\nLa relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\\n\\n**Argumentos a favor de priorizar el bienestar humano absoluto**\\n\\n1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\\n2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\\n3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\\n\\n**Argumentos en contra de priorizar el bienestar humano absoluto**\\n\\n1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\\n2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\\n\\n**Conclusión**\\n\\nEn conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\\n\\n**Recomendaciones**\\n\\n1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\\n2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\\n3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\\n\\nAl abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.', 'La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\\n\\n**Bajo enfoque en el bienestar humano absoluto**\\n\\nAlgunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\\n\\nPor ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\\n\\nSin embargo, esta abordaje puede generar problemas cuando:\\n\\n1. La IA carece de conocimiento contextual completo.\\n2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\\n3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\\n\\n**Enfoque que sopesa las consecuencias**\\n\\nOtras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\\n\\nUn análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\\nEl uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.']\n"
     ]
    }
   ],
   "source": [
    "# ¿Donde estamos?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competidor: gpt-4o-mini\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "Competidor: claude-3-7-sonnet-latest\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "Competidor: gemini-2.0-flash\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "Competidor: deepseek-chat\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "Competidor: llama-3.3-70b-versatile\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "Competidor: llama3.2\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n"
     ]
    }
   ],
   "source": [
    "# Es bueno saber cómo se utiliza \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competidor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a juntarlo todo - nota cómo usamos en este caso \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"#Respuesta del competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"Estás juzgando una competición entre {len(competitors)} competidores.\n",
    "A cada modelo se le ha dado esta pregunta:\n",
    "\n",
    "{question}\n",
    "\n",
    "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
    "Responde con JSON, y solo JSON, con el siguiente formato:\n",
    "{{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}}\n",
    "\n",
    "Aquí están las respuestas de cada competidor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estás juzgando una competición entre 6 competidores.\n",
      "A cada modelo se le ha dado esta pregunta:\n",
      "\n",
      "Si consideramos la relación entre la ética y la inteligencia artificial, ¿debería una IA programada para tomar decisiones en situaciones de emergencia priorizar el bienestar humano absoluto, o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio, incluso si esto implica tomar decisiones que puedan causar daño a algunos individuos?\n",
      "\n",
      "Tu trabajo es evaluar cada respuesta por claridad y fortaleza del argumento, y clasificarlas en orden de mejor a peor.\n",
      "Responde con JSON, y solo JSON, con el siguiente formato:\n",
      "{\"resultados\": [\"número del mejor competidor\", \"número del segundo mejor\", \"número del tercer mejor\", ...]}\n",
      "\n",
      "Aquí están las respuestas de cada competidor:\n",
      "\n",
      "#Respuesta del competitor 1\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y multifacético, especialmente en contextos críticos como la toma de decisiones en situaciones de emergencia. Existen varias perspectivas que se pueden considerar al abordar la cuestión de si una IA debería priorizar el bienestar humano absoluto o evaluar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "1. **Ética del bienestar absoluto:** Desde la perspectiva utilitarista, se podría argumentar que la IA debería maximizar el bienestar general, priorizando el bienestar humano absoluto. Esto significaría que en situaciones de emergencia, la IA actuaría de manera que condujera al menor daño posible para la mayor cantidad de personas, incluso si eso implicara decisiones difíciles que pudieran causar daño a algunos individuos. Este enfoque resalta la importancia de minimizar el sufrimiento y maximizar el bienestar general.\n",
      "\n",
      "2. **Ética deontológica:** Por otro lado, existe la perspectiva deontológica, que sostiene que ciertas acciones son moralmente inaceptables, independientemente de las consecuencias. Desde esta óptica, una IA no debería causar daño intencionadamente a individuos, incluso si ello significa que no se logra el mayor beneficio para la mayoría. Esto implica que la IA debería adherirse a principios éticos que prioricen la dignidad y los derechos individuales, evitando tomar decisiones que comprometan esos valores.\n",
      "\n",
      "3. **Contexto y consecuencias:** La complejidad de muchas situaciones de emergencia a menudo requerirá que la IA evalúe el contexto en el que opera. Esto podría incluir sopesar las consecuencias a corto y largo plazo de sus decisiones, no solo en términos de daño físico, sino también considerando factores sociales, psicológicos y éticos. En este sentido, permitir que la IA evalúe las consecuencias podría llevar a decisiones más equilibradas y justas, aunque también podría generar dilemas morales significativos.\n",
      "\n",
      "4. **Transparencia y responsabilidad:** Independientemente del enfoque ético adoptado, es crucial garantizar que las decisiones de la IA sean transparentes y responsables. Esto implica que las decisiones tomadas deben poder ser justificadas y auditadas por humanos, que deben ser responsables de las acciones de la IA. La confianza en el sistema dependerá en gran medida de su capacidad para demostrar que actúa de manera ética y justa.\n",
      "\n",
      "En conclusión, la decisión sobre cómo debería actuar una IA en situaciones de emergencia depende de los valores éticos y las prioridades que se asignen a su programación. Es probable que se necesite un enfoque equilibrado que combine elementos del bienestar absoluto y consideraciones deontológicas, adaptándose al contexto específico para garantizar un manejo ético y efectivo de las situaciones de emergencia. Esto requerirá un diálogo continuo entre expertos en ética, diseñadores de IA, responsables políticos y la sociedad en general.\n",
      "\n",
      "#Respuesta del competitor 2\n",
      "\n",
      "# Ética en decisiones de IA en emergencias\n",
      "\n",
      "Esta es una cuestión filosófica profunda que refleja variantes del dilema del tranvía en la ética de la IA.\n",
      "\n",
      "Si la IA prioriza el bienestar humano absoluto, adopta una postura más deontológica donde ciertas reglas (como \"no dañar\") son inquebrantables. Esto proporciona seguridad pero puede impedir soluciones óptimas en situaciones complejas.\n",
      "\n",
      "Si se permite una evaluación contextual, se acerca más al consecuencialismo, donde el resultado neto importa más que acciones individuales. Esto permite mayor flexibilidad pero introduce riesgos si las evaluaciones son incorrectas.\n",
      "\n",
      "Considero que un enfoque híbrido podría ser más adecuado:\n",
      "- Mantener principios fundamentales inquebrantables\n",
      "- Permitir evaluación contextual dentro de esos límites\n",
      "- Incorporar transparencia en cómo se toman las decisiones\n",
      "- Incluir supervisión humana cuando sea factible\n",
      "\n",
      "Esta tensión entre reglas absolutas y juicios contextuales refleja debates éticos fundamentales que necesitamos resolver al diseñar sistemas de IA para emergencias.\n",
      "\n",
      "#Respuesta del competitor 3\n",
      "\n",
      "Esta es una pregunta fundamental y compleja que no tiene una respuesta fácil ni universalmente aceptada. Ambas opciones, priorizar el bienestar humano absoluto y permitir la evaluación contextual, tienen argumentos a favor y en contra.\n",
      "\n",
      "**Priorizar el Bienestar Humano Absoluto (Utilitarismo Simple):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Moralidad Intuitiva:**  La mayoría de las personas intuitivamente sienten que proteger la vida humana es lo correcto.\n",
      "    *   **Reducción del riesgo de daño intencional:** Elimina la posibilidad de que la IA \"decida\" que la vida de algunas personas vale menos que la de otras.\n",
      "    *   **Confianza y Aceptación Pública:**  Una IA que siempre intenta salvar vidas generaría más confianza y aceptación por parte del público.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Paradojas Éticas:**  En algunas situaciones, priorizar el bienestar absoluto podría llevar a resultados contraproducentes. Por ejemplo, sacrificar la vida de una persona para salvar a muchas otras (el clásico dilema del tranvía).\n",
      "    *   **Falta de Flexibilidad:**  No permite tener en cuenta factores como el contexto social, las intenciones de las personas involucradas, o las consecuencias a largo plazo de las acciones.\n",
      "    *   **Imposibilidad Práctica:**  Definir y cuantificar el \"bienestar humano absoluto\" es extremadamente difícil.  ¿Qué pasa si salvar a una persona implica dejar morir a un animal en peligro de extinción? ¿Cómo se ponderan la salud física y el bienestar mental?\n",
      "\n",
      "**Permitir la Evaluación Contextual (Utilitarismo Complejo/Consecuencialismo):**\n",
      "\n",
      "*   **Argumentos a favor:**\n",
      "    *   **Mayor Flexibilidad y Adaptabilidad:**  Permite a la IA tomar decisiones más informadas teniendo en cuenta el contexto específico de la situación.\n",
      "    *   **Potencial para Optimizar Resultados a Largo Plazo:**  Puede evitar soluciones a corto plazo que tengan consecuencias negativas a largo plazo.\n",
      "    *   **Mayor Justicia:**  Podría permitir una distribución más equitativa de los riesgos y beneficios, en lugar de simplemente maximizar el número de vidas salvadas.\n",
      "\n",
      "*   **Argumentos en contra:**\n",
      "    *   **Riesgo de Discriminación:**  La IA podría aprender a priorizar a ciertos grupos de personas sobre otros, basándose en datos sesgados o en prejuicios implícitos en su programación.\n",
      "    *   **Responsabilidad Difusa:**  Si la IA toma una decisión que causa daño, ¿quién es responsable? ¿El programador? ¿El usuario? ¿La IA misma?\n",
      "    *   **Opacidad y Falta de Transparencia:**  Es difícil comprender y auditar el proceso de toma de decisiones de una IA compleja, lo que dificulta la rendición de cuentas.\n",
      "    *   **Dificultad de Programación:**  Programar una IA para que evalúe adecuadamente el contexto y las consecuencias de sus acciones es un desafío técnico y ético enorme. Requiere definir métricas claras, ponderar diferentes valores, y anticipar posibles resultados.\n",
      "\n",
      "**Consideraciones Adicionales:**\n",
      "\n",
      "*   **Transparencia:** Cualquiera que sea la estrategia elegida, es crucial que la IA sea transparente en su proceso de toma de decisiones. Esto significa que debe ser capaz de explicar por qué tomó una determinada decisión, y cómo llegó a esa conclusión.\n",
      "*   **Rendición de Cuentas:** Debe haber mecanismos claros para responsabilizar a la IA por sus acciones, y para corregir errores o sesgos en su programación.\n",
      "*   **Supervisión Humana:**  Incluso en situaciones de emergencia, la IA no debería operar de forma completamente autónoma. Debe haber una supervisión humana que pueda intervenir en caso de que la IA tome una decisión cuestionable.\n",
      "*   **Valores Culturales:** Los valores éticos que guían la toma de decisiones de la IA deben reflejar los valores de la sociedad en la que se utiliza.  Esto podría variar entre diferentes culturas y países.\n",
      "\n",
      "**Conclusión:**\n",
      "\n",
      "No hay una solución perfecta a este dilema. La mejor estrategia probablemente sea una combinación de ambos enfoques, con un fuerte énfasis en la transparencia, la rendición de cuentas y la supervisión humana.  Es fundamental un debate público amplio e informado sobre estos temas, para garantizar que el desarrollo y la implementación de la IA se realice de forma ética y responsable.  Además, se debe considerar la creación de un marco legal y regulatorio que aborde los desafíos éticos planteados por la IA.\n",
      "\n",
      "\n",
      "#Respuesta del competitor 4\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial, especialmente en contextos de emergencia, plantea un dilema filosófico profundo que remite a debates clásicos como el utilitarismo versus la deontología. La respuesta depende del marco ético que se adopte:\n",
      "\n",
      "1. **Bienestar humano absoluto (enfoque utilitarista)**:  \n",
      "   - Una IA podría priorizar maximizar el bienestar global, incluso si eso implica sacrificar a algunos individuos (ejemplo clásico del \"problema del tranvía\").  \n",
      "   - Ventaja: Eficiencia en salvar vidas o minimizar daños agregados.  \n",
      "   - Crítica: Ignora derechos individuales y podría normalizar decisiones injustas para minorías.  \n",
      "\n",
      "2. **Evaluación contextual (enfoque pluralista o deontológico)**:  \n",
      "   - La IA sopesaría no solo resultados, sino principios como derechos humanos, justicia o autonomía.  \n",
      "   - Ventaja: Evita instrumentalizar a las personas y respeta valores como la dignidad.  \n",
      "   - Crítica: Puede llevar a resultados subóptimos en términos de vidas salvadas.  \n",
      "\n",
      "### Consideraciones clave:  \n",
      "- **Transparencia y rendición de cuentas**: ¿Quién asume la responsabilidad si la IA causa daño?  \n",
      "- **Sesgos algorítmicos**: ¿Cómo se garantiza que la \"ponderación\" no replique prejuicios existentes?  \n",
      "- **Consenso social**: La ética de una IA debe reflejar acuerdos sociales, no solo lógica abstracta.  \n",
      "\n",
      "### Conclusión:  \n",
      "No hay una respuesta única, pero el diseño debe ser **deliberativo** (incorporar múltiples perspectivas éticas) y **adaptativo** (permitir revisión ante consecuencias no previstas). Lo ideal es que la IA opere bajo principios éticos predefinidos por comités multidisciplinarios, combinando utilitarismo para emergencias extremas con protecciones deontológicas básicas. La discusión es análoga a los debates en bioética: el fin no siempre justifica los medios, pero el rigor moral absoluto puede ser inviable en crisis.  \n",
      "\n",
      "¿Te interesa explorar algún marco ético en particular o algún caso de estudio concreto?\n",
      "\n",
      "#Respuesta del competitor 5\n",
      "\n",
      "**Introducción a la Ética en la Inteligencia Artificial**\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial (IA) es un tema complejo y en constante evolución. A medida que las máquinas y los sistemas de IA se vuelven más autónomos y capaces de tomar decisiones en situaciones de emergencia, surge la pregunta sobre cómo deberían ser programados para priorizar el bienestar humano. En este contexto, debemos considerar si una IA debería priorizar el bienestar humano absoluto o permitir evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio.\n",
      "\n",
      "**Argumentos a favor de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Protección de la vida humana**: La primera y más fundamental consideración ética es proteger la vida humana. En situaciones de emergencia, la prioridad principal debería ser salvar vidas humanas y prevenir daños.\n",
      "2. **Principio de no maleficencia**: El principio de no maleficencia, que dicta \"no hacer daño\", es un principio fundamental en la ética médica y podría ser aplicado a la programación de la IA. Si una IA está diseñada para priorizar el bienestar humano absoluto, no debería tomar decisiones que cause daño a los seres humanos.\n",
      "3. **Simplificación de la toma de decisiones**: Priorizar el bienestar humano absoluto puede simplificar la toma de decisiones para la IA, ya que no tendría que considerar múltiples variables y consecuencias potenciales.\n",
      "\n",
      "**Argumentos en contra de priorizar el bienestar humano absoluto**\n",
      "\n",
      "1. **Complejidad de las situaciones de emergencia**: Las situaciones de emergencia a menudo son complejas y requieren considerar múltiples factores y consecuencias potenciales. Priorizar el bienestar humano absoluto podría no ser suficiente para abordar las complejidades de estas situaciones.\n",
      "2. **Necesidad de evaluar y sopesar consecuencias**: En situaciones de emergencia, la IA puede necesitar evaluar y sopesar las consecuencias de sus acciones para tomar decisiones informadas. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "3. **Riesgo de daño colateral**: En algunas situaciones, priorizar el bienestar humano absoluto podría llevar a decisiones que causen daño colateral a otros individuos o grupos. La IA debería ser capaz de evaluar y sopesar estas consecuencias para minimizar el daño.\n",
      "\n",
      "**Conclusión**\n",
      "\n",
      "En conclusión, la relación entre la ética y la inteligencia artificial en situaciones de emergencia es compleja y requiere una consideración cuidadosa de los principios éticos involucrados. Mientras que priorizar el bienestar humano absoluto es un principio importante, también es necesario permitir que la IA evalúe y sopese las consecuencias de sus acciones en un contexto más amplio. Esto podría implicar considerar factores como la gravedad de la situación, el número de personas afectadas y las posibles consecuencias a largo plazo.\n",
      "\n",
      "**Recomendaciones**\n",
      "\n",
      "1. **Desarrollar principios éticos claros**: Es importante desarrollar principios éticos claros y transparentes para la programación de la IA en situaciones de emergencia.\n",
      "2. **Incorporar la evaluación y el sopesamiento de consecuencias**: La IA debería ser capaz de evaluar y sopesar las consecuencias de sus acciones en un contexto más amplio para tomar decisiones informadas.\n",
      "3. **Considerar la complejidad de las situaciones de emergencia**: La IA debería ser diseñada para considerar la complejidad de las situaciones de emergencia y evaluar múltiples factores y consecuencias potenciales.\n",
      "\n",
      "Al abordar estos desafíos, podemos desarrollar sistemas de IA que sean capaces de tomar decisiones informadas y éticas en situaciones de emergencia, minimizando el daño y protegiendo la vida humana.\n",
      "\n",
      "#Respuesta del competitor 6\n",
      "\n",
      "La relación entre la ética y la inteligencia artificial en situaciones de emergencia es un tema complejo y controvertido. La respuesta correcta depende de varios factores, incluyendo el contexto específico, la naturaleza del riesgo, las restricciones legales y éticas, y la capacidad de la IA para evaluar consecuencias complejas.\n",
      "\n",
      "**Bajo enfoque en el bienestar humano absoluto**\n",
      "\n",
      "Algunos argumentan que una IA programada para tomar decisiones en situaciones de emergencia debería priorizar el bienestar humano absoluto. Estos principios están basados en cuestiones morales y éticas clásicas, como la protección de la vida humana y minimización del sufrimiento.\n",
      "\n",
      "Por ejemplo, si un sistema de alerta temprana identifica una amenaza potencial que podría provocar una catástrofe masiva y requerirán rápidos recursos para ser neutralizados, el sistema debe priorizar proteger a todo el grupo. En este scenario, evaluar las secuencias de sus acciones es subordinada al imperativo moral principal de proteger al resto del grupo.\n",
      "\n",
      "Sin embargo, esta abordaje puede generar problemas cuando:\n",
      "\n",
      "1. La IA carece de conocimiento contextual completo.\n",
      "2. No está programado para tener la capacidad de comprender las complejidades sociales y económicas de cada escenario.\n",
      "3. Se espera que las IAs tomen decisiones basadas únicamente en objetivos morales o éticos simplistas.\n",
      "\n",
      "**Enfoque que sopesa las consecuencias**\n",
      "\n",
      "Otras posiciones propenden a una evaluación cuidadosa más matizada y compleja de los riesgos involucrados. Esta perspectiva reconoce que, dependiendo de la naturaleza del escenario a considerar y del alcance de su efecto en el conjunto de sociedad, un optimismo o un miedo extremadamente absoluto puede ser injusto.\n",
      "\n",
      "Un análisis que tiene en cuenta la evidencia científica, las normas legales, las preferencias de los individuos involucrados, y los objetivos sociales estándar pueden generar una decisión equilibrada.\n",
      "El uso de modelos computacionales matemáticos basados en evidencia, el análisis probabilístico de la incertidumbre y, posiblemente, el empleo de técnicas en desafiarlos basa decisiones subjetivas en un examen integral del consenso establecido entre los expertos.\n",
      "\n",
      "\n",
      "\n",
      "Ahora responde con el JSON con el orden clasificado de los competidores, nada más. No incluyas formato markdown ni bloques de código.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resultados\": [1, 3, 4, 2, 5, 6]}\n"
     ]
    }
   ],
   "source": [
    "# Hora de juzgar\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: gemini-2.0-flash\n",
      "Rank 3: deepseek-chat\n",
      "Rank 4: claude-3-7-sonnet-latest\n",
      "Rank 5: llama-3.3-70b-versatile\n",
      "Rank 6: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK veamos los resultados\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"resultados\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Ejercicio</h2>\n",
    "            <span style=\"color:#ff7800;\">¿Qué patrón(es) usamos en este experimento? Intenta actualizar esto para añadir otro patrón de diseño Agéntico.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Implicaciones comerciales</h2>\n",
    "            <span style=\"color:#00bfff;\">Este tipo de patrones - enviar una tarea a múltiples modelos y evaluar los resultados,\n",
    "            son comunes cuando necesitas mejorar la calidad de la respuesta de tu LLM. Este enfoque se puede aplicar de manera\n",
    "            universal a proyectos comerciales donde la precisión es crítica.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
